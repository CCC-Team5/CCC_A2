{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bf6e4b1-aa4f-4dc5-988a-dc43210c6071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/weimin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/weimin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import couchdb\n",
    "# from couchdb_settings import *\n",
    "import geojson\n",
    "from geojson import Point, Feature, FeatureCollection, dump\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d290e07-c190-4701-93be-64e6e178e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# couchdb_settings\n",
    "address = '172.26.134.35:5984'\n",
    "username = 'grp5admin'\n",
    "password = 'password'\n",
    "tweets = 'raw_tweets'\n",
    "user = 'user_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daf82f24-e85c-423f-b3ce-f8c896e80cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_connect(dbname):\n",
    "    \"\"\"\n",
    "    connect to CouchDB server to create databases\n",
    "    params: name of the database to be created\n",
    "    return: created database\n",
    "    \"\"\"\n",
    "\n",
    "    couchserver = couchdb.Server('http://' + username + ':' + password + '@' + address)\n",
    "    try:\n",
    "      db = couchserver[dbname]\n",
    "    except:\n",
    "      db = couchserver.create(dbname)\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8289750f-4d32-4e01-a10e-1596fcd99898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_DB(dbname):\n",
    "    \"\"\"\n",
    "    connect to CouchDB\n",
    "    params: name of the database to connect to\n",
    "    return: the db to establish connection with\n",
    "    return type: database\n",
    "    \"\"\"\n",
    "    \n",
    "    db = couchdb.Database('http://' + address + '/' + dbname)\n",
    "    db.resource.credentials = (username, password)\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63d6bdbf-9661-4acc-bfc9-6aa6571eff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to raw_tweets db\n",
    "tweet_db = fetch_DB(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6654c0b-166c-479d-b415-ced42d3e1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_docs(topic, save_db):\n",
    "    \"\"\"\n",
    "    delete existing data in DB (to replace previous data analyses results)\n",
    "    params: the topic to look at (one of housing, cost or transportation);\n",
    "            the database where the particular topic results were saved in\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = []    \n",
    "    for row in save_db.view(topic + '/all', include_docs=True):\n",
    "        doc = row['doc']\n",
    "        if int(doc['year']) >= 2018:\n",
    "            doc['_deleted']=True\n",
    "            docs.append(doc)\n",
    "        save_db.update(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b67b1cfd-512c-401e-ada1-4f7fed1f1df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tiktok': 96,\n",
       " 'alboforpm': 107,\n",
       " 'gopies': 114,\n",
       " 'armukrainenow': 124,\n",
       " 'victraffic': 135,\n",
       " 'slavaukraini': 147,\n",
       " 'melbourne': 165,\n",
       " 'ausvotes': 190,\n",
       " 'auspol': 495,\n",
       " 'امپورٹڈ_حکومت_نامنظور': 918}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def now_trending(db, N):\n",
    "    \"\"\"\n",
    "    extract top N mostly used hasgtags in tweets from past 14 days\n",
    "    params: raw_tweets database;\n",
    "            number of hashtags to extract\n",
    "    return: top N hashtags extracted from tweets made within last 14 days; \n",
    "            all hashtags in lowercases\n",
    "    return type: dict - {hashtag:count}\n",
    "    frontend: wordcloud\n",
    "    \"\"\"\n",
    "    \n",
    "    hashtags = {}\n",
    "    \n",
    "    for item in db.view('hashtags/trending', group = True, group_level = 1):\n",
    "        if item.key.lower() not in hashtags.keys():\n",
    "            hashtags[item.key.lower()] = item.value\n",
    "        else:\n",
    "            hashtags[item.key.lower()] += item.value\n",
    "        \n",
    "    hashtags = {k: v for k, v in sorted(hashtags.items(), key=lambda item: item[1])[-N:]}\n",
    "\n",
    "    return hashtags\n",
    "\n",
    "# run now_trending\n",
    "now_trending(tweet_db, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68109eee-c87b-41de-abed-a02e687022a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = fetch_DB('langcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdd21dd1-85ef-4fb2-b168-ff9334100125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langCode(db):\n",
    "\n",
    "    langCode = {}\n",
    "    for item in db.view('lang/Code'):\n",
    "        langCode[item.key] = item.value\n",
    "        \n",
    "    return langCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03a9f974-bae3-481c-8efc-5d6778231823",
   "metadata": {},
   "outputs": [],
   "source": [
    "langCode = read_langCode(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88b82dcd-d41c-411c-aeec-1dbfc6ead469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'af': 'Afrikaans',\n",
       " 'am': 'Amharic',\n",
       " 'ar': 'Arabic',\n",
       " 'arn': 'Mapudungun',\n",
       " 'as': 'Assamese',\n",
       " 'az': 'Azeri',\n",
       " 'ba': 'Bashkir',\n",
       " 'be': 'Belarusian',\n",
       " 'bg': 'Bulgarian',\n",
       " 'bn': 'Bengali',\n",
       " 'bo': 'Tibetan',\n",
       " 'br': 'Breton',\n",
       " 'bs': 'Bosnian',\n",
       " 'ca': 'Catalan',\n",
       " 'co': 'Corsican',\n",
       " 'cs': 'Czech',\n",
       " 'cy': 'Welsh',\n",
       " 'da': 'Danish',\n",
       " 'de': 'German',\n",
       " 'dsb': 'Lower-Sorbian',\n",
       " 'el': 'Greek',\n",
       " 'en': 'English',\n",
       " 'es': 'Spanish',\n",
       " 'et': 'Estonian',\n",
       " 'eu': 'Basque',\n",
       " 'fa': 'Persian',\n",
       " 'fi': 'Finnish',\n",
       " 'fil': 'Filipino',\n",
       " 'fo': 'Faroese',\n",
       " 'fr': 'French',\n",
       " 'fy': 'Frisian',\n",
       " 'ga': 'Irish',\n",
       " 'gd': 'Scottish-Gaelic',\n",
       " 'gl': 'Galician',\n",
       " 'gsw': 'Alsatian',\n",
       " 'gu': 'Gujarati',\n",
       " 'ha': 'Hausa',\n",
       " 'he': 'Hebrew',\n",
       " 'hi': 'Hindi',\n",
       " 'hr': 'Croatian',\n",
       " 'hsb': 'Upper-Sorbian',\n",
       " 'ht': 'Haitian',\n",
       " 'hu': 'Hungarian',\n",
       " 'hy': 'Armenian',\n",
       " 'id': 'Indonesian',\n",
       " 'ig': 'Igbo',\n",
       " 'ii': 'Yi',\n",
       " 'is': 'Icelandic',\n",
       " 'it': 'Italian',\n",
       " 'iu': 'Inuktitut',\n",
       " 'ja': 'Japanese',\n",
       " 'ka': 'Georgian',\n",
       " 'kk': 'Kazakh',\n",
       " 'kl': 'Greenlandic',\n",
       " 'km': 'Khmer',\n",
       " 'kn': 'Kannada',\n",
       " 'ko': 'Korean',\n",
       " 'kok': 'Konkani',\n",
       " 'ky': 'Kyrgyz',\n",
       " 'lb': 'Luxembourgish',\n",
       " 'lo': 'Lao',\n",
       " 'lt': 'Lithuanian',\n",
       " 'lv': 'Latvian',\n",
       " 'mi': 'Maori',\n",
       " 'mk': 'Macedonian',\n",
       " 'ml': 'Malayalam',\n",
       " 'mn': 'Mongolian',\n",
       " 'moh': 'Mohawk',\n",
       " 'mr': 'Marathi',\n",
       " 'msa': 'Malay',\n",
       " 'mt': 'Maltese',\n",
       " 'my': 'Burmese',\n",
       " 'ne': 'Nepali',\n",
       " 'nl': 'Dutch',\n",
       " 'no': 'Norwegian',\n",
       " 'nso': 'Sesotho',\n",
       " 'oc': 'Occitan',\n",
       " 'or': 'Oriya',\n",
       " 'pa': 'Punjabi',\n",
       " 'pl': 'Polish',\n",
       " 'prs': 'Dari',\n",
       " 'ps': 'Pashto',\n",
       " 'pt': 'Portuguese',\n",
       " 'qut': \"K'iche\",\n",
       " 'quz': 'Quechua',\n",
       " 'rm': 'Romansh',\n",
       " 'ro': 'Romanian',\n",
       " 'ru': 'Russian',\n",
       " 'rw': 'Kinyarwanda',\n",
       " 'sa': 'Sanskrit',\n",
       " 'sah': 'Yakut',\n",
       " 'se': 'Sami-Northern',\n",
       " 'si': 'Sinhala',\n",
       " 'sk': 'Slovak',\n",
       " 'sl': 'Slovenian',\n",
       " 'sma': 'Sami-Southern',\n",
       " 'smj': 'Sami-Lule',\n",
       " 'smn': 'Sami-Lnari',\n",
       " 'sms': 'Sami-Skolt',\n",
       " 'sq': 'Albanian',\n",
       " 'sr': 'Serbian',\n",
       " 'sv': 'Swedish',\n",
       " 'sw': 'Kiswahili',\n",
       " 'syr': 'Syriac',\n",
       " 'ta': 'Tamil',\n",
       " 'te': 'Telugu',\n",
       " 'tg': 'Tajik',\n",
       " 'th': 'Thai',\n",
       " 'tk': 'Turkmen',\n",
       " 'tl': 'Tagalog',\n",
       " 'tn': 'Setswana',\n",
       " 'tr': 'Turkish',\n",
       " 'tt': 'Tatar',\n",
       " 'tzm': 'Tamazight',\n",
       " 'ug': 'Uyghur',\n",
       " 'uk': 'Ukrainian',\n",
       " 'ur': 'Urdu',\n",
       " 'uz': 'Uzbek',\n",
       " 'vi': 'Vietnamese',\n",
       " 'wo': 'Wolof',\n",
       " 'xh': 'isiXhosa',\n",
       " 'yo': 'Yoruba',\n",
       " 'zh': 'Chinese',\n",
       " 'zu': 'isiZulu'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb259a0d-5e8c-4484-ba6d-487b646d2daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Japanese': 11960,\n",
       " 'Spanish': 11814,\n",
       " 'Indonesian': 8785,\n",
       " 'Arabic': 6030,\n",
       " 'Tagalog': 4549,\n",
       " 'French': 3316,\n",
       " 'Chinese': 3111,\n",
       " 'Portuguese': 2853,\n",
       " 'Thai': 2763,\n",
       " 'Turkish': 2133}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_n_lang_count(db, langCode_db, N):\n",
    "    \"\"\"\n",
    "    extract top N languages other than English in which tweets were made\n",
    "    params: raw_tweets database;\n",
    "            path to langCode.json file;\n",
    "            number of languages to extract\n",
    "    return: top N most tweeted languages other than English\n",
    "    return type: dict - {language code: count}\n",
    "    frontend: bar chart/pie chart (colour matching for most tweeted languages/counrty of birth/language spoken at home)\n",
    "    \"\"\"\n",
    "    languages = {}\n",
    "    \n",
    "    for item in db.view('lang/lang-count', group = True, group_level = 1):\n",
    "        if item.key != 'en':\n",
    "            if item.key == 'in':\n",
    "                languages['id'] = item.value\n",
    "            else:\n",
    "                languages[item.key] = item.value\n",
    "            \n",
    "        languages = {k:v for k, v in sorted(languages.items(), key=lambda item: item[1])[::-1][:N]}\n",
    "    \n",
    "    langCode = read_langCode(langCode_db)\n",
    "    \n",
    "    languages = {v2: v1 for k1, v1 in languages.items() for k2, v2 in langCode.items() if k1 == k2}\n",
    "            \n",
    "    return languages\n",
    "\n",
    "# run top n languages\n",
    "top_n_lang_count(tweet_db, db, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af40c817-13a2-4a1d-996a-9385a55bc4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "birthdb = fetch_DB('birthcountry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "234dd530-a4a5-415d-ac48-1c4fbd391b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Germany': [17511, 0.38707478361743725],\n",
       " 'Pakistan': [19127, 0.42279592177778097],\n",
       " 'Philippines': [43642, 0.9646917769763117],\n",
       " 'Greece': [43881, 0.9699747918403725],\n",
       " 'Malaysia': [45852, 1.0135430859703463],\n",
       " 'Sri Lanka': [52658, 1.1639874339402094],\n",
       " 'Italy': [61521, 1.3599010772045204],\n",
       " 'Vietnam': [78036, 1.7249596147775874],\n",
       " 'India': [160058, 3.5380284230620616],\n",
       " 'China': [174418, 3.85545140819977]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_n_birth_country(db, N):\n",
    "\n",
    "    birth = {}\n",
    "    for item in db.view('birth/country'):\n",
    "        birth[item.key] = item.value\n",
    "        \n",
    "    birth = {k: v for k, v in sorted(birth.items(), key=lambda item: item[1])[-N:]}\n",
    "    \n",
    "    return birth\n",
    "\n",
    "top_n_birth_country(birthdb, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14772fd3-9c7d-4aef-907e-9e30dc00008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "homelang = fetch_DB('homelang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f1da22f-493f-46d5-8dbb-3759d11b228c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Spanish': [27632, 2.376169510181618, 0.6908029359124777],\n",
       " 'Macedonian': [29378, 2.526313979086406, 0.734453121425766],\n",
       " 'Punjabi': [29517, 2.5382670610897082, 0.7379281361945789],\n",
       " 'Turkish': [31257, 2.6878955696202533, 0.7814283210703645],\n",
       " 'Hindi': [31607, 2.717993258117777, 0.7901783582580226],\n",
       " 'Arabic': [65454, 5.628611722619703, 1.6363569545170566],\n",
       " 'Vietnamese': [85122, 7.319929829389103, 2.1280590442509384],\n",
       " 'Italian': [112665, 9.68844592735278, 2.8166369707071257],\n",
       " 'Greek': [113424, 9.753714914694552, 2.835612051351218],\n",
       " 'Chinese': [189854, 16.326190148596588, 4.746370172073231]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_n_lang_spoken_at_home(db, N):\n",
    "\n",
    "    spoken = {}\n",
    "    for item in db.view('home/lang'):\n",
    "        spoken[item.key] = item.value\n",
    "        \n",
    "    spoken = {k: v for k, v in sorted(spoken.items(), key=lambda item: item[1])[-N:]}\n",
    "    \n",
    "    return spoken\n",
    "\n",
    "top_n_lang_spoken_at_home(homelang, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10ca648f-e13a-429c-a065-8c929a7a9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_switch(topic):\n",
    "    \"\"\"\n",
    "    params: topic of selection\n",
    "    return: paths to the views relating to the selected topic;\n",
    "            connect to db where top related results were saved\n",
    "    \"\"\"\n",
    "\n",
    "    count_view = 'text/' + topic + '-count'\n",
    "    topic_view = 'text/' + topic\n",
    "\n",
    "    return count_view, topic_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fd09876-8079-40d2-ade4-0aa7a13faec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_trend(db, topic):\n",
    "    \"\"\"\n",
    "    extract the number and percentage of tweets on the selected topic made each year\n",
    "    params: raw_tweets database;\n",
    "            the topic of selection\n",
    "    return:  \n",
    "    return type: dict - {year : number of tweets on the selected topic made in that year}\n",
    "                 dict - {year : total number of tweets made in that year}\n",
    "                 dict - {year : percentage of tweets on selected topic over total number of tweets made in that year}\n",
    "    frontend: Dual axes, line and column (combine with topic sentiment as the line)\n",
    "    \"\"\"\n",
    "\n",
    "    count_view, _ = topic_switch(topic)\n",
    "    \n",
    "    year_topic = {}\n",
    "    year_total = {}\n",
    "    percent = {}\n",
    "\n",
    "    for item in db.view(count_view, group = True, group_level = 1):\n",
    "        year_topic[item.key] = item.value\n",
    "\n",
    "    for item in db.view('time/by-year-count', group = True, group_level = 1):\n",
    "        year_total[item.key] = item.value\n",
    "\n",
    "    for key in year_topic.keys():\n",
    "        percent[key] = year_topic[key]/year_total[key] * 100\n",
    "            \n",
    "    return year_topic, year_total, percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abeabe91-1b27-4397-bb26-be590c409301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({2014: 165, 2015: 154, 2016: 7, 2017: 365, 2018: 8, 2019: 18, 2020: 50, 2021: 168, 2022: 187}, {2014: 211755, 2015: 198258, 2016: 10801, 2017: 107505, 2018: 9586, 2019: 15241, 2020: 35793, 2021: 86473, 2022: 99981}, {2014: 0.07792023801090883, 2015: 0.07767656286253266, 2016: 0.06480881399870382, 2017: 0.33951909213524956, 2018: 0.08345503859795535, 2019: 0.11810248671347025, 2020: 0.13969211857067024, 2021: 0.19428029558359255, 2022: 0.18703553675198287})\n",
      "({2014: 4, 2015: 5, 2017: 4, 2019: 1, 2021: 4, 2022: 37}, {2014: 211755, 2015: 198258, 2016: 10801, 2017: 107505, 2018: 9586, 2019: 15241, 2020: 35793, 2021: 86473, 2022: 99981}, {2014: 0.0018889754669311233, 2015: 0.0025219663267056058, 2017: 0.0037207571740849265, 2019: 0.006561249261859458, 2021: 0.00462572132341887, 2022: 0.03700703133595383})\n",
      "({2014: 792, 2015: 1044, 2016: 93, 2017: 2439, 2018: 15, 2019: 59, 2020: 57, 2021: 119, 2022: 567}, {2014: 211755, 2015: 198258, 2016: 10801, 2017: 107505, 2018: 9586, 2019: 15241, 2020: 35793, 2021: 86473, 2022: 99981}, {2014: 0.3740171424523624, 2015: 0.5265865690161304, 2016: 0.8610313859827794, 2017: 2.268731686898284, 2018: 0.15647819737116628, 2019: 0.387113706449708, 2020: 0.15924901517056406, 2021: 0.1376152093717114, 2022: 0.5671077504725899})\n"
     ]
    }
   ],
   "source": [
    "topics = ['housing', 'cost', 'transportation']\n",
    "\n",
    "for topic in topics:\n",
    "    trend = topic_trend(tweet_db, topic)\n",
    "    print(trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aeff426c-e409-4572-a23e-58efa6cc8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_wordcloud_save(query_db, topic, save_db):\n",
    "\n",
    "    # fetch the view to access & connect to the database where the topic related results were saved\n",
    "    _, topic_view = topic_switch(topic)\n",
    "\n",
    "    # if there were results saved from previous data analyses runs, delete \n",
    "    # (because there would have been new data collected by the Twitter-Harvester so the results would have changed)\n",
    "    try:\n",
    "        delete_docs(topic, save_db)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    yearly_tweets = defaultdict(list)\n",
    "    for item in query_db.view(topic_view):\n",
    "        if int(item.key) >= 2018:\n",
    "            yearly_tweets[item.key].append(item.value)\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    \n",
    "    for key, tweet in yearly_tweets.items():\n",
    "        tweet = [' '.join(re.sub(\"(@[A-Za-z0-9\\_]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",t).split()) for t in tweet]\n",
    "        tweet = [' '.join(tweet)]\n",
    "        tweet_tokens = tokenizer.tokenize(tweet[0])\n",
    "        tweet_clean = []\n",
    "        for word in tweet_tokens:\n",
    "            if word.lower() not in stopwords.words('english') and word.lower() not in string.punctuation:\n",
    "                tweet_clean.append(word.lower())\n",
    "                \n",
    "        yearly_tweets[key] = ' '.join(tweet_clean)\n",
    "\n",
    "    # save data analyses results to the topic related results database for the backend to directly access through views\n",
    "    for k, v in yearly_tweets.items():\n",
    "        save_db.save({'year': k, 'text':v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60670a57-af43-461c-b894-f84a29b4875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run topic wordcloud\n",
    "for topic in topics:\n",
    "    save_db = db_connect(topic + '_text') \n",
    "    topic_wordcloud_save(tweet_db, topic, save_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3dff39b-1af7-4adb-ab23-39d7e25ff0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_word_cloud(topic):\n",
    "    \"\"\"\n",
    "    extract topic related wordcloud\n",
    "    params: topic related text database;\n",
    "            topic of selection\n",
    "    return: corpus of combined tweets on the selected topic indexed by year; \n",
    "            all words in lowercases\n",
    "    return type: dict - {year : corpus as a list}\n",
    "    frontend: wordcloud\n",
    "    \"\"\"\n",
    "\n",
    "    yearly_tweets = {}\n",
    "    for item in db.view(topic + '/text'):\n",
    "        yearly_tweets[item.key] = item.value   \n",
    "\n",
    "    return yearly_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e582d074-bff9-4c36-9e9b-fcef16ca880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topics:\n",
    "    db = fetch_DB(topic + '_text')\n",
    "    topic_word_cloud(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a2f88a8-4283-4168-95ba-972c34279b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_sentiment(topic):\n",
    "    \"\"\"\n",
    "    extract topic related sentiment\n",
    "    params: topic of selection\n",
    "    return: sentiment towards the selected topic indexed by year\n",
    "    return type: dict - {year : sentiment score}\n",
    "    render: Dual axes, line and column (combine with topic trend as the columns)\n",
    "    \"\"\"\n",
    "    \n",
    "    yearly_tweets = {}\n",
    "    for item in db.view(topic + '/text'):\n",
    "        yearly_tweets[item.key] = item.value\n",
    "    \n",
    "    yearly_sentiment = {}\n",
    "    for key, value in yearly_tweets.items():\n",
    "        blob = TextBlob(value)\n",
    "        for sentence in blob.sentences:\n",
    "            sentiment = sentence.sentiment.polarity\n",
    "            yearly_sentiment[key] = sentiment\n",
    "\n",
    "    return yearly_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0df28c0c-b775-43d4-b4a4-5738a8a87d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2014: 0.19313898143741073, 2015: 0.07747157991060426, 2016: 0.22619047619047622, 2017: 0.07160565243311717, 2018: 0.10800000000000001, 2019: 0.13429682929682932, 2020: 0.10835478680611424, 2021: 0.10943601545630192, 2022: 0.10005737447910346}\n",
      "{2014: 0.21095238095238095, 2015: -0.1708333333333333, 2017: -0.02857142857142858, 2019: 0.1375, 2021: 0.02727272727272726, 2022: 0.05202380952380951}\n",
      "{2014: -0.0066004984047371386, 2015: -0.013641858141858245, 2016: -0.005781702037799617, 2017: -0.032537428863271586, 2018: 0.2530917280917281, 2019: 0.05319404621352675, 2020: 0.08493145743145744, 2021: 0.06922077922077922, 2022: -0.014736664490401105}\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    db = fetch_DB(topic + '_text')\n",
    "    print(topic_sentiment(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bd51ab2-48f1-4e5f-95d2-66b196fadf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend to access data analyses results directly from CouchDB:\n",
    "# e.g topic: housing - other available topics: cost, transportation\n",
    "\n",
    "db = fetch_DB(topic + '_text') # fetch the database where the topic related results were saved\n",
    "\n",
    "yearly_tweets = {}\n",
    "for item in db.view(topic + '/text'):\n",
    "    yearly_tweets[item.key] = item.value\n",
    "\n",
    "# results returned as a dict in the form of {year: corpus}\n",
    "# fetch the corpus corresponding to the selected year, or combine all corpus into one, to render for the frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d063ae7c-af99-433d-9f06-511dd010914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_LatLong(db):\n",
    "    \"\"\"\n",
    "    extract langitude and longitude information if tweets contain the information\n",
    "    params: raw_tweets database\n",
    "    return: coordinates of tweets that contain the information\n",
    "    return type: geojson\n",
    "    frontend: map\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for item in db.view('geo/new-view'):\n",
    "        cor = item.key\n",
    "        features.append(Feature(geometry=Point((cor[0], cor[1]))))\n",
    "\n",
    "    feature_collection = FeatureCollection(features)\n",
    "#     with open('myfile1.geojson', 'w') as f:\n",
    "#         dump(feature_collection, f)\n",
    "    return feature_collection\n",
    "geo_LatLong(tweet_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113f90c-4128-46a6-845f-f128841debff",
   "metadata": {},
   "source": [
    "## For Data Processing ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f12e50-fea3-4e29-bfe1-63a93c5c6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_langCode(langCode_path):\n",
    "    \"\"\"\n",
    "    param: language code file path\n",
    "    return: {language_code: language_name} - language code dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    langCode_db = db_connect('langcode')\n",
    "    with open(langCode_path, 'r', encoding= 'utf-8') as f:\n",
    "        for line in f:\n",
    "            (val, key) = line.split()\n",
    "            langCode_db.save({'code': key, 'language':val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "16954cf7-e66d-45fc-aa45-7951e8195f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def birth_country(file_path):\n",
    "    \"\"\"\n",
    "    extract top N non-English-speaking countries where people living in the Greater Melbourne were originally from\n",
    "    params: path to census data download from AURIN - 'country_of_birth.csv';\n",
    "            number of non-English-speaking countries to extract\n",
    "    return: top N non-English-speaking countries' names, total population count, and percentage population\n",
    "    return type: numpy arrays\n",
    "    frontend: bar chart/pie chart (colour matching for most tweeted languages/counrty of birth/language spoken at home)\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    match_cols = []\n",
    "    new_cols = []\n",
    "    col_names = data.columns\n",
    "    for name in col_names:\n",
    "        if name.endswith('_p'):\n",
    "            match_cols.append(name)\n",
    "            new_cols.append(name.strip())\n",
    "\n",
    "    ext_data = pd.DataFrame(data[match_cols])\n",
    "    ext_data.columns = new_cols\n",
    "    \n",
    "    grand_total = ext_data['tot_p'].sum(axis = 0)\n",
    "    country_total = ext_data.sum(axis = 0)\n",
    "    percentage = country_total/grand_total * 100\n",
    "    \n",
    "    birth_country = pd.DataFrame(country_total, columns = ['country_total'])\n",
    "    birth_country['percentage'] = percentage\n",
    "    \n",
    "    drop_columns = ['hong_kong_sar_china_p', 'born_elsewhere_p', 'tot_p', 'os_visitors_p', 'country_birth_not_stated_p', 'australia_p', 'new_zealand_p', 'united_states_america_p', 'united_kingdom_ci_im_p', 'fiji_p', 'south_africa_p']\n",
    "    \n",
    "    birth_country.loc['china_excl_sars_taiwan_p'] += birth_country.loc['hong_kong_sar_china_p']\n",
    "    birth_country = birth_country.T.drop(drop_columns, axis = 1)\n",
    "    birth_country = birth_country.rename({'china_excl_sars_taiwan_p' : 'china_p', 'sri_lanka_p' : 'srilanka_p'}, axis = 1)\n",
    "    \n",
    "    country_names = []\n",
    "    for item in birth_country.columns:\n",
    "        item = item.split('_')\n",
    "        country_names.append(item[0].capitalize())\n",
    "\n",
    "    birth_country.columns = country_names\n",
    "    birth_country = birth_country.rename({'Srilanka' : 'Sri Lanka'}, axis = 1).T\n",
    "    birth_country = birth_country.sort_values(by = ['country_total'], ascending = False)\n",
    "\n",
    "    birthcountry_db = db_connect('birthcountry')\n",
    "    for i in range(len(birth_country)):\n",
    "        birthcountry_db.save({'country': birth_country.index[i], 'values' : (birth_country.country_total.values[i], birth_country.percentage.values[i])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c90bef46-0a89-4ddd-97c5-85fa318c7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_spoken_at_home(file_path):\n",
    "    \"\"\"\n",
    "    extract top N languages other than English spoken at home\n",
    "    params: path to census data download from AURIN - 'lang_at_home.csv';\n",
    "            path to langCode.json file;\n",
    "            number of languages other than English to extract\n",
    "    return: names of top N languages other than English spoken at home, total population count, percentage of population count to total SOL population, \n",
    "            percentage of population count to total population, and percentage of SOL population to total population\n",
    "    return type: numpy arrays\n",
    "    frontend: bar chart/pie chart (colour matching for most tweeted languages/counrty of birth/language spoken at home)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    match_cols = []\n",
    "    new_cols = []\n",
    "    col_names = data.columns\n",
    "    for name in col_names:\n",
    "        if name.endswith('_P'):\n",
    "            match_cols.append(name)   \n",
    "            new_cols.append(name.strip())\n",
    "\n",
    "    ext_data = pd.DataFrame(data[match_cols])\n",
    "    ext_data.columns = new_cols\n",
    "\n",
    "    SOL_tot = ext_data['SOL_Tot_P'].sum(axis = 0)\n",
    "    tot = ext_data['Total_P'].sum(axis = 0)\n",
    "    SOL_perc = SOL_tot/tot * 100\n",
    "\n",
    "    drop_columns = ['SOL_Other_P', 'SOL_Samoan_P', 'SOL_Assyrian_P','SOL_Iran_Lan_Tot_P', \n",
    "                    'SOL_Irani_Lan_Othr_P', 'SOL_Se_As_A_L_Othr_P', 'SOL_Aus_Indig_Lang_P', \n",
    "                    'SOL_In_Ar_Lang_Othr_P', 'SOL_In_Ar_Lang_Tot_P', 'SOL_Se_As_A_L_Tot_P', \n",
    "                    'Language_spoken_home_ns_P', 'SOL_Tot_P', 'Total_P', \n",
    "                    'SOL_Chin_lang_Mand_P', 'SOL_Chin_lang_Other_P', 'SOL_Chin_lang_Cant_P']\n",
    "    \n",
    "    ext_data = ext_data.drop(drop_columns, axis = 1)\n",
    "    lang_tot = ext_data.sum(axis = 0)\n",
    "    \n",
    "    columns = []\n",
    "    for index in lang_tot.index:\n",
    "        idx = index.split('_')\n",
    "        if 'Se_As' in index or 'In_Ar' in index or 'Ir_Lang' in index:\n",
    "            columns.append(idx[-2])\n",
    "        elif 'Ir_La' in index:\n",
    "            columns.append(idx[3])\n",
    "        else:\n",
    "            columns.append(idx[1])\n",
    "\n",
    "    lang_tot.index = columns\n",
    "    lang_data = pd.DataFrame(lang_tot, columns = ['number']).T\n",
    "    lang_data = lang_data.rename({'Pe' : 'Persian'}, axis = 1).T\n",
    "    lang_data['percentage_SOL'] = lang_data['number']/SOL_tot * 100\n",
    "    lang_data['percentage_Total'] = lang_data['number']/tot * 100\n",
    "\n",
    "    langCode = read_langCode()\n",
    "    langdict = {k:v for v in langCode.values() for k in lang_data.index if k in v}\n",
    "\n",
    "    idx = []\n",
    "    for i in lang_data.index:\n",
    "        name = langdict[i]\n",
    "        idx.append(name)\n",
    "\n",
    "    lang_data.index = idx\n",
    "    lang_data = lang_data.sort_values(by = ['number'], ascending = False)\n",
    "    \n",
    "    homelang_db = db_connect('homelang')\n",
    "    for i in range(len(lang_data)):\n",
    "        array = np.array([lang_data.number.values[i], lang_data.percentage_SOL.values[i], lang_data.percentage_Total.values[i]])\n",
    "        values = array.tolist()\n",
    "        homelang_db.save({'country': lang_data.index[i], 'values' : values})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
