{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf6e4b1-aa4f-4dc5-988a-dc43210c6071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/weimin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/weimin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import couchdb\n",
    "# from couchdb_settings import *\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from textblob import TextBlob\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d290e07-c190-4701-93be-64e6e178e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# couchdb_settings\n",
    "address = '172.26.130.201:5984' \n",
    "username = 'grp5admin'\n",
    "password = 'password'\n",
    "tweets = 'raw_tweets'\n",
    "user = 'user_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf82f24-e85c-423f-b3ce-f8c896e80cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_connect(dbname):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    couchserver = couchdb.Server('http://' + username + ':' + password + '@' + address)\n",
    "    try:\n",
    "      db = couchserver[dbname]\n",
    "    except:\n",
    "      db = couchserver.create(dbname)\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8289750f-4d32-4e01-a10e-1596fcd99898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_DB(dbname):\n",
    "    \"\"\"\n",
    "    connect to CouchDB\n",
    "    params: credentials, db addressm dbname\n",
    "    return: the db to establish connection with\n",
    "    return type: database\n",
    "    \"\"\"\n",
    "    \n",
    "    db = couchdb.Database('http://' + address + '/' + dbname)\n",
    "    db.resource.credentials = (username, password)\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63d6bdbf-9661-4acc-bfc9-6aa6571eff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to raw_tweets db\n",
    "tweet_db = fetch_DB(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6654c0b-166c-479d-b415-ced42d3e1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_docs(topic, save_db):\n",
    "    \"\"\"\n",
    "    delete existing data in DB\n",
    "    \"\"\"\n",
    "    \n",
    "    # use this after DB is completely ready\n",
    "    # docs = []    \n",
    "    # for row in save_db.view(topic + '/all', include_docs=True):\n",
    "    #     doc = row['doc']\n",
    "    #     if int(doc['year'] >=2018):\n",
    "    #         doc['_deleted']=True\n",
    "    #         docs.append(doc)\n",
    "    #     save_db.update(docs)\n",
    "        \n",
    "    docs = []    \n",
    "    for row in save_db.view(topic + '/all', include_docs=True):\n",
    "        doc = row['doc']\n",
    "        doc['_deleted']=True\n",
    "        docs.append(doc)\n",
    "        save_db.update(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b67b1cfd-512c-401e-ada1-4f7fed1f1df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'afl': 107,\n",
       " 'alboforpm': 111,\n",
       " 'gopies': 117,\n",
       " 'armukrainenow': 124,\n",
       " 'slavaukraini': 147,\n",
       " 'victraffic': 150,\n",
       " 'melbourne': 172,\n",
       " 'ausvotes': 190,\n",
       " 'auspol': 511,\n",
       " 'امپورٹڈ_حکومت_نامنظور': 918}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def now_trending(db, N):\n",
    "    \"\"\"\n",
    "    Extract top N mostly used hasgtags in tweets from past 14 days\n",
    "    params: raw_tweets database;\n",
    "            number of hashtags to extract\n",
    "    return: top N hashtags extracted from tweets made within last 14 days; \n",
    "            all hashtags in lowercases\n",
    "    return type: dict - {hashtag:count}\n",
    "    render: wordcloud\n",
    "    \"\"\"\n",
    "    \n",
    "    hashtags = {}\n",
    "    \n",
    "    for item in db.view('hashtags/trending', group = True, group_level = 1):\n",
    "        if item.key.lower() not in hashtags.keys():\n",
    "            hashtags[item.key.lower()] = item.value\n",
    "        else:\n",
    "            hashtags[item.key.lower()] += item.value\n",
    "        \n",
    "    hashtags = {k: v for k, v in sorted(hashtags.items(), key=lambda item: item[1])[-N:]}\n",
    "\n",
    "    return hashtags\n",
    "\n",
    "now_trending(tweet_db, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49cb971f-9a84-422d-8f5c-6e7403885c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langCode(langCode_path):\n",
    "    \"\"\"\n",
    "    param: language code file path\n",
    "    return: {language_code: language_name} - language code dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    langCode = {}\n",
    "    with open(langCode_path, 'r', encoding= 'utf-8') as f:\n",
    "        for line in f:\n",
    "            (val, key) = line.split()\n",
    "            langCode[key] = val\n",
    "    return langCode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e9efc45-73f3-48d1-b72e-caa4e7d858e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "langCode_path = 'Data/langCode.json'\n",
    "\n",
    "# read_langCode(langCode_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb259a0d-5e8c-4484-ba6d-487b646d2daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Indonesian': 23201,\n",
       " 'Spanish': 22505,\n",
       " 'Japanese': 17309,\n",
       " 'Arabic': 12625,\n",
       " 'Turkish': 9132,\n",
       " 'Tagalog': 8936,\n",
       " 'French': 6675,\n",
       " 'Portuguese': 4943,\n",
       " 'Chinese': 4317,\n",
       " 'Thai': 4232}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_n_lang_count(db, langCode_path, N):\n",
    "    \"\"\"\n",
    "    Extract top N languages other than English in which tweets were made\n",
    "    params: raw_tweets database;\n",
    "            path to langCode.json file;\n",
    "            number of languages to extract\n",
    "    return: top N most tweeted languages other than English\n",
    "    return type: dict - {language code: count}\n",
    "    render: Bar chart?\n",
    "    \"\"\"\n",
    "    languages = {}\n",
    "    \n",
    "    for item in db.view('lang/lang-count', group = True, group_level = 1):\n",
    "        if item.key != 'en':\n",
    "            if item.key == 'in':\n",
    "                languages['id'] = item.value\n",
    "            else:\n",
    "                languages[item.key] = item.value\n",
    "            \n",
    "        languages = {k:v for k, v in sorted(languages.items(), key=lambda item: item[1])[::-1][:N]}\n",
    "    \n",
    "    langCode = read_langCode(langCode_path)\n",
    "    \n",
    "    languages = {v2: v1 for k1, v1 in languages.items() for k2, v2 in langCode.items() if k1 == k2}\n",
    "            \n",
    "    return languages\n",
    "\n",
    "top_n_lang_count(tweet_db, langCode_path, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16954cf7-e66d-45fc-aa45-7951e8195f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_birth_country(file_path, N):\n",
    "    \"\"\"\n",
    "    Extract top N non-English-speaking countries where people living in the Greater Melbourne were originally from\n",
    "    params: path to census data download from AURIN - 'country_of_birth.csv';\n",
    "            number of non-English-speaking countries to extract\n",
    "    return: top N non-English-speaking countries' names, total population count, and percentage population\n",
    "    return type: numpy arrays\n",
    "    render: Pareto chart? Bar chart?\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    match_cols = []\n",
    "    new_cols = []\n",
    "    col_names = data.columns\n",
    "    for name in col_names:\n",
    "        if name.endswith('_p'):\n",
    "            match_cols.append(name)\n",
    "            new_cols.append(name.strip())\n",
    "\n",
    "    ext_data = pd.DataFrame(data[match_cols])\n",
    "    ext_data.columns = new_cols\n",
    "    \n",
    "    grand_total = ext_data['tot_p'].sum(axis = 0)\n",
    "    country_total = ext_data.sum(axis = 0)\n",
    "    percentage = country_total/grand_total * 100\n",
    "    \n",
    "    birth_country = pd.DataFrame(country_total, columns = ['country_total'])\n",
    "    birth_country['percentage'] = percentage\n",
    "    \n",
    "    drop_columns = ['hong_kong_sar_china_p', 'born_elsewhere_p', 'tot_p', 'os_visitors_p', 'country_birth_not_stated_p', 'australia_p', 'new_zealand_p', 'united_states_america_p', 'united_kingdom_ci_im_p', 'fiji_p', 'south_africa_p']\n",
    "    \n",
    "    birth_country.loc['china_excl_sars_taiwan_p'] += birth_country.loc['hong_kong_sar_china_p']\n",
    "    birth_country = birth_country.T.drop(drop_columns, axis = 1)\n",
    "    birth_country = birth_country.rename({'china_excl_sars_taiwan_p' : 'china_p', 'sri_lanka_p' : 'srilanka_p'}, axis = 1)\n",
    "    \n",
    "    country_names = []\n",
    "    for item in birth_country.columns:\n",
    "        item = item.split('_')\n",
    "        country_names.append(item[0].capitalize())\n",
    "\n",
    "    birth_country.columns = country_names\n",
    "    birth_country = birth_country.rename({'Srilanka' : 'Sri Lanka'}, axis = 1).T\n",
    "    birth_country = birth_country.sort_values(by = ['country_total'], ascending = False)[:N]\n",
    "\n",
    "    birth = {}\n",
    "    for i in range(len(birth_country)):\n",
    "        birth[birth_country.index[i]] = birth_country.country_total.values[i], birth_country.percentage.values[i]\n",
    "    \n",
    "    return birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c77f538b-fda1-465f-8de9-7ab239d3162e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'China': (174418.0, 3.85545140819977),\n",
       " 'India': (160058.0, 3.5380284230620616),\n",
       " 'Vietnam': (78036.0, 1.7249596147775874),\n",
       " 'Italy': (61521.0, 1.3599010772045204),\n",
       " 'Sri Lanka': (52658.0, 1.1639874339402094),\n",
       " 'Malaysia': (45852.0, 1.0135430859703463),\n",
       " 'Greece': (43881.0, 0.9699747918403725),\n",
       " 'Philippines': (43642.0, 0.9646917769763117),\n",
       " 'Pakistan': (19127.0, 0.42279592177778097),\n",
       " 'Germany': (17511.0, 0.38707478361743725)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'Data/AURIN/country_of_birth.csv'\n",
    "top_n_birth_country(filepath, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c90bef46-0a89-4ddd-97c5-85fa318c7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_lang_spoken_at_home(file_path, langCode_path, N):\n",
    "    \"\"\"\n",
    "    Extract top N languages other than English spoken at home\n",
    "    params: path to census data download from AURIN - 'lang_at_home.csv';\n",
    "            path to langCode.json file;\n",
    "            number of languages other than English to extract\n",
    "    return: names of top N languages other than English spoken at home, total population count, percentage of population count to total SOL population, \n",
    "            percentage of population count to total population, and percentage of SOL population to total population\n",
    "    return type: numpy arrays\n",
    "    render: Pareto chart? Bar chart?\n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    match_cols = []\n",
    "    new_cols = []\n",
    "    col_names = data.columns\n",
    "    for name in col_names:\n",
    "        if name.endswith('_P'):\n",
    "            match_cols.append(name)   \n",
    "            new_cols.append(name.strip())\n",
    "\n",
    "    ext_data = pd.DataFrame(data[match_cols])\n",
    "    ext_data.columns = new_cols\n",
    "\n",
    "    SOL_tot = ext_data['SOL_Tot_P'].sum(axis = 0)\n",
    "    tot = ext_data['Total_P'].sum(axis = 0)\n",
    "    SOL_perc = SOL_tot/tot * 100\n",
    "\n",
    "    drop_columns = ['SOL_Other_P', 'SOL_Samoan_P', 'SOL_Assyrian_P','SOL_Iran_Lan_Tot_P', \n",
    "                    'SOL_Irani_Lan_Othr_P', 'SOL_Se_As_A_L_Othr_P', 'SOL_Aus_Indig_Lang_P', \n",
    "                    'SOL_In_Ar_Lang_Othr_P', 'SOL_In_Ar_Lang_Tot_P', 'SOL_Se_As_A_L_Tot_P', \n",
    "                    'Language_spoken_home_ns_P', 'SOL_Tot_P', 'Total_P', \n",
    "                    'SOL_Chin_lang_Mand_P', 'SOL_Chin_lang_Other_P', 'SOL_Chin_lang_Cant_P']\n",
    "    \n",
    "    ext_data = ext_data.drop(drop_columns, axis = 1)\n",
    "    lang_tot = ext_data.sum(axis = 0)\n",
    "    \n",
    "    columns = []\n",
    "    for index in lang_tot.index:\n",
    "        idx = index.split('_')\n",
    "        if 'Se_As' in index or 'In_Ar' in index or 'Ir_Lang' in index:\n",
    "            columns.append(idx[-2])\n",
    "        elif 'Ir_La' in index:\n",
    "            columns.append(idx[3])\n",
    "        else:\n",
    "            columns.append(idx[1])\n",
    "\n",
    "    lang_tot.index = columns\n",
    "    lang_data = pd.DataFrame(lang_tot, columns = ['number']).T\n",
    "    lang_data = lang_data.rename({'Pe' : 'Persian'}, axis = 1).T\n",
    "    lang_data['percentage_SOL'] = lang_data['number']/SOL_tot * 100\n",
    "    lang_data['percentage_Total'] = lang_data['number']/tot * 100\n",
    "\n",
    "    langCode = read_langCode(langCode_path)\n",
    "    langdict = {k:v for v in langCode.values() for k in lang_data.index if k in v}\n",
    "\n",
    "    idx = []\n",
    "    for i in lang_data.index:\n",
    "        name = langdict[i]\n",
    "        idx.append(name)\n",
    "\n",
    "    lang_data.index = idx\n",
    "    lang_data = lang_data.sort_values(by = ['number'], ascending = False)[:N]\n",
    "    \n",
    "    spoken = {}\n",
    "    for i in range(len(lang_data)):\n",
    "        array = np.array([lang_data.number.values[i], lang_data.percentage_SOL.values[i], lang_data.percentage_Total.values[i]])\n",
    "        spoken[lang_data.index[i]] = array.tolist()\n",
    "\n",
    "    return spoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81fb2f96-b92c-4456-b971-6353f27166d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Chinese': [189854.0, 16.326190148596588, 4.746370172073231],\n",
       " 'Greek': [113424.0, 9.753714914694552, 2.835612051351218],\n",
       " 'Italian': [112665.0, 9.68844592735278, 2.8166369707071257],\n",
       " 'Vietnamese': [85122.0, 7.319929829389103, 2.1280590442509384],\n",
       " 'Arabic': [65454.0, 5.628611722619703, 1.6363569545170566],\n",
       " 'Hindi': [31607.0, 2.717993258117777, 0.7901783582580226],\n",
       " 'Turkish': [31257.0, 2.6878955696202533, 0.7814283210703645],\n",
       " 'Punjabi': [29517.0, 2.5382670610897082, 0.7379281361945789],\n",
       " 'Macedonian': [29378.0, 2.526313979086406, 0.734453121425766],\n",
       " 'Spanish': [27632.0, 2.376169510181618, 0.6908029359124777]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'Data/AURIN/lang_at_home.csv'\n",
    "top_n_lang_spoken_at_home(filepath, langCode_path, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bb3f4e9-1e31-4db3-858d-c391c5a85949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_switch(topic):\n",
    "    \"\"\"\n",
    "    params: topic of selection\n",
    "    return: paths to views relating to the selected topic \n",
    "    \"\"\"\n",
    "\n",
    "    count_view = 'text/' + topic + '-count'\n",
    "    topic_view = 'text/' + topic\n",
    "    topic_db = db_connect(topic + '_text')\n",
    "\n",
    "    return count_view, topic_view, topic_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6265b8b-3c39-4925-95b5-fc2371acd019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_trend(db, topic):\n",
    "    \"\"\"\n",
    "    Extract the number and percentage of tweets on the selected topic made each year\n",
    "    params: raw_tweets database;\n",
    "            the topic of selection\n",
    "    return:  \n",
    "    return type: dict - {year : number of tweets on the selected topic made in that year}\n",
    "                 dict - {year : total number of tweets made in that year}\n",
    "                 dict - {year : percentage of tweets on selected topic over total number of tweets made in that year}\n",
    "    render: Dual axes, line and column (combine with topic sentiment as the line)\n",
    "    \"\"\"\n",
    "\n",
    "    count_view, _, _ = topic_switch(topic)\n",
    "    \n",
    "    year_topic = {}\n",
    "    year_total = {}\n",
    "    percent = {}\n",
    "\n",
    "    for item in db.view(count_view, group = True, group_level = 1):\n",
    "        year_topic[item.key] = item.value\n",
    "\n",
    "    for item in db.view('time/by-year-count', group = True, group_level = 1):\n",
    "        year_total[item.key] = item.value\n",
    "\n",
    "    for key in year_topic.keys():\n",
    "        percent[key] = year_topic[key]/year_total[key] * 100\n",
    "            \n",
    "    return year_topic, year_total, percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36fe809e-14e0-454f-89b3-fc7b6b555a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['housing', 'cost', 'transportation']\n",
    "\n",
    "trend = topic_trend(tweet_db, 'transportation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee385fdf-acf3-4653-b9fd-8769aab9f92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({2019: 1, 2021: 4, 2022: 37},\n",
       " {2014: 36319,\n",
       "  2015: 1,\n",
       "  2018: 9586,\n",
       "  2019: 15241,\n",
       "  2020: 35793,\n",
       "  2021: 86473,\n",
       "  2022: 99981},\n",
       " {2019: 0.006561249261859458,\n",
       "  2021: 0.00462572132341887,\n",
       "  2022: 0.03700703133595383})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "027746dd-70bd-4d18-aead-b397cf2490fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_wordcloud(query_db, topic):\n",
    "    \"\"\"\n",
    "    Extract topic related wordcloud\n",
    "    params: raw_tweets database;\n",
    "            topic of selection\n",
    "    return: corpus of combined tweets on the selected topic indexed by year; \n",
    "            all words in lowercases\n",
    "    return type: dict - {year : corpus as a list}\n",
    "    render: wordcloud\n",
    "    \"\"\"\n",
    "\n",
    "    _, topic_view, save_db = topic_switch(topic)\n",
    "\n",
    "    try:\n",
    "        delete_docs(topic, save_db)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    yearly_tweets = defaultdict(list)\n",
    "    for item in query_db.view(topic_view):\n",
    "        yearly_tweets[item.key].append(item.value)\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    \n",
    "    for key, tweet in yearly_tweets.items():\n",
    "        tweet = [' '.join(re.sub(\"(@[A-Za-z0-9\\_]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",t).split()) for t in tweet]\n",
    "        tweet = [' '.join(tweet)]\n",
    "        tweet_tokens = tokenizer.tokenize(tweet[0])\n",
    "        tweet_clean = []\n",
    "        for word in tweet_tokens:\n",
    "            if word.lower() not in stopwords.words('english') and word.lower() not in string.punctuation:\n",
    "                tweet_clean.append(word.lower())\n",
    "                \n",
    "        yearly_tweets[key] = ' '.join(tweet_clean)\n",
    "\n",
    "    for k, v in yearly_tweets.items():\n",
    "        save_db.save({'year': k, 'text':v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b44f77b-5b9e-4528-90ad-2fd9e2b95654",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topics:\n",
    "    topic_wordcloud(tweet_db, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7bb62db1-512a-458c-8eb0-f3a398543a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_sentiment(topic):\n",
    "    \"\"\"\n",
    "    Extract topic related sentiment\n",
    "    params: raw_tweets database;\n",
    "            topic of selection\n",
    "    return: sentiment towards the selected topic indexed by year\n",
    "    return type: dict - {year : sentiment score}\n",
    "    render: Dual axes, line and column (combine with topic trend as the columns)\n",
    "    \"\"\"\n",
    "\n",
    "    _, _, db = topic_switch(topic)\n",
    "\n",
    "    yearly_tweets = {}\n",
    "    for item in db.view(topic + '/text'):\n",
    "        yearly_tweets[item.key] = item.value\n",
    "    \n",
    "    yearly_sentiment = {}\n",
    "    for key, value in yearly_tweets.items():\n",
    "        blob = TextBlob(value)\n",
    "        for sentence in blob.sentences:\n",
    "            sentiment = sentence.sentiment.polarity\n",
    "            yearly_sentiment[key] = sentiment\n",
    "\n",
    "    return yearly_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e2a0f7c0-0e5f-455a-ac2b-d273c2ae0422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2014: 0.2710676351140269, 2018: 0.10800000000000001, 2019: 0.13429682929682932, 2020: 0.10835478680611424, 2021: 0.10943601545630192, 2022: 0.10005737447910346}\n",
      "{2014: 0.16666666666666666, 2019: 0.1375, 2021: 0.02727272727272726, 2022: 0.05202380952380951}\n",
      "{2014: -0.015116756259613354, 2018: 0.2530917280917281, 2019: 0.05319404621352675, 2020: 0.08493145743145744, 2021: 0.06922077922077922, 2022: -0.014736664490401105}\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    print(topic_sentiment(topic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
