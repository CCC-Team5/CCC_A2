{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf366b5-a8cb-4ca1-8580-66dd83ed55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# couchbd_settings\n",
    "address = 'localhost:15984'\n",
    "username = 'admin'\n",
    "password = '123456'\n",
    "# tweets = 'raw_tweets'\n",
    "tweets = 'test'\n",
    "user = 'user_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665ec6aa-db78-4af9-b727-2a9f8d2aa6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "import couchdb\n",
    "import re\n",
    "class CouchDBHandler:\n",
    "    def __init__(self, username, password, address, dbname):\n",
    "        self.db = couchdb.Server('http://' + username + ':' + password + '@' + address)[dbname]\n",
    "        \n",
    "    def get_tweets(self, max_size):\n",
    "        tweet_historic_dict = defaultdict(list)\n",
    "        for i, doc_id in enumerate(self.db.view('_all_docs')):\n",
    "            if max_size:\n",
    "                if i > max_size:\n",
    "                    break\n",
    "            id_ = doc_id['id']\n",
    "            try:\n",
    "                year = self.db[id_]['historic']['created_at']\n",
    "                if year[:2] == '20':\n",
    "                    yr_created = year[:4]\n",
    "                elif year[-4:-2] == '20':\n",
    "                    yr_created = year[-4:]\n",
    "                else:  \n",
    "                    yr_created = 'Unknown' \n",
    "                \n",
    "                tweet_historic_dict[yr_created].append(self.db[id_]['historic']['text'])\n",
    "            except:\n",
    "                print(\"error:\", self.db[id_])\n",
    "        return tweet_historic_dict \n",
    "    \n",
    "    #def send_tweets(tweets: dict[str, str]):\n",
    "        # self.db.save(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "344cd7f7-3d84-403d-a5c8-26847dee280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_handler = CouchDBHandler(username, password, address, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d705aca7-c6d2-4164-a687-4983f1591787",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_historic_dict = db_handler.get_tweets(max_size=341556)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f51f2d45-ce5f-45a3-8676-d895bf04f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('tweet_historical.json', 'w') as fp:\n",
    "    json.dump(tweet_historic_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3210fc66-d0eb-45f6-9751-a6714d09d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# couchbd_settings\n",
    "address = 'localhost:15984'\n",
    "username = 'admin'\n",
    "password = '123456'\n",
    "tweets2 = 'raw_tweets'\n",
    "user = 'user_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45805abf-9eb4-4890-b8fa-b9d6235d55c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from collections import defaultdict\n",
    "import couchdb\n",
    "import re\n",
    "class CouchDBHandler:\n",
    "    def __init__(self, username, password, address, dbname):\n",
    "        self.db = couchdb.Server('http://' + username + ':' + password + '@' + address)[dbname]\n",
    "        \n",
    "    def get_tweets(self, max_size):\n",
    "        tweet_timeline_dict = defaultdict(list)\n",
    "        tweet_stream_dict = defaultdict(list)\n",
    "        for i, doc_id in enumerate(self.db.view('_all_docs')):\n",
    "            if max_size:\n",
    "                if i > max_size:\n",
    "                    break\n",
    "            id_ = doc_id['id']\n",
    "            try:\n",
    "                year = self.db[id_]['stream']['created_at']\n",
    "                if year[:2] == '20':\n",
    "                    yr_created = year[:4]\n",
    "                elif year[-4:-2] == '20':\n",
    "                    yr_created = year[-4:]\n",
    "                else:  \n",
    "                    yr_created = 'Unknown' \n",
    "                \n",
    "                tweet_stream_dict[yr_created].append(self.db[id_]['stream']['text'])\n",
    "            except:\n",
    "                year = self.db[id_]['timeline']['created_at']\n",
    "                \n",
    "                if year[:2] == '20':\n",
    "                    yr_created = year[:4]\n",
    "                else:  #close to today\n",
    "                    yr_created = '2022' \n",
    "                \n",
    "                tweet_timeline_dict[yr_created].append(self.db[id_]['timeline']['text'])\n",
    "        return tweet_timeline_dict, tweet_stream_dict\n",
    "    \n",
    "    #def send_tweets(tweets: dict[str, str]):\n",
    "        # self.db.save(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d9d7e17-0565-4ec6-a94d-0b5264ab0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_handler2 = CouchDBHandler(username, password, address, tweets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e84578b-a00d-4787-bd01-7db3f9094815",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_timeline_dict,tweet_stream_dict = db_handler2.get_tweets(max_size=145112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a53ac113-82dc-4588-ab52-f34e1d33bc39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('tweet_timeline.json', 'w') as fp:\n",
    "    json.dump(tweet_timeline_dict, fp)\n",
    "with open('tweet_stream.json', 'w') as fp:\n",
    "    json.dump(tweet_stream_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8a9de-d377-42ad-866a-8d4d6cd6c637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72e3c690-270b-463a-9790-13c6a3bdce14",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b858be83-c212-4517-862c-2a3061b8b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('tweet_timeline.json', 'w') as fp:\n",
    "    json.dump(tweet_timeline_dict, fp)\n",
    "with open('tweet_stream.json', 'w') as fp:\n",
    "    json.dump(tweet_stream_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89dbfb2e-1971-4385-8c38-c75737568299",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def f(s, pat):\n",
    "    pat = r'(\\w*%s\\w*)' % pat # Not thrilled about this line\n",
    "    return re.findall(pat, s)\n",
    "\n",
    "print(f(\"This is just a simple text to test some basic things\", \"#\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00be904-572d-4268-b79d-962c481fe1cd",
   "metadata": {},
   "source": [
    "## historic data: hash words and words count by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1faba6b-c259-4e82-95d6-edf0af57350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a347e1c8-9999-40f8-9a03-d1d36aedd4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common hash words: [('#Melbourne', 3245), ('#melbourne', 1893), ('#auspol', 869), ('#Job', 753), ('#Jobs', 559), ('#victraffic', 479), ('#TweetMyJobs', 424), ('#', 378), ('#australia', 361), ('#Australia', 333), ('#firewatch', 293), ('#AUSvIND', 286), ('#qanda', 259), ('#vicvotes', 253), ('#NashsNewVideo', 246), ('#vote5sos', 235), ('#coffee', 226), ('#love', 223), ('#springst', 218), ('#FollowMeNash', 218), ('#illridewithyou', 203), ('#foodporn', 201), ('#food', 191), ('#vscocam', 181), ('#SaveDallas', 162), ('#krazykristmas', 161), ('#travel', 157), ('#AFLGF', 157), ('#photo', 156), ('#BusinessMgmt', 155), ('#Christmas', 153), ('#summer', 153), ('#christmas', 151), ('#jackfollowme', 148), ('#sydneysiege', 146), ('#TheBachelorAU', 128), ('#nofilter', 127), ('#beach', 127), ('#Sales', 126), ('#GetCamTo3Mill', 122), ('#sunset', 119), ('#art', 118), ('#subscribetokianandjc', 118), ('#yum', 117), ('#spring', 117), ('#stkilda', 116), ('#SmallzyWelcomesHome5SOS', 116), ('#ihgjobs', 115), ('#ihg', 115), ('#hoteljobs', 115)]\n",
      "common words: [(\"'s\", 18924), ('melbourne', 13865), (\"n't\", 12830), (\"'m\", 9785), ('...', 9096), ('amp', 8367), ('https', 7751), (\"''\", 6848), ('like', 6790), ('``', 6667), ('love', 6515), ('day', 6245), ('good', 5958), ('get', 5581), ('..', 5370), ('one', 5298), ('time', 4918), ('today', 4886), ('vic', 4393), ('go', 4348), ('great', 4049), ('new', 3905), ('would', 3892), ('please', 3767), ('follow', 3659), ('see', 3532), ('u', 3522), ('going', 3372), ('know', 3369), ('timeline', 3300)]\n",
      "common hash words: [('#Melbourne', 2441), ('#nowplaying', 1736), ('#melbourne', 1401), ('#Incident', 720), ('#Job', 544), ('#StructureFire', 487), ('#auspol', 482), ('#Jobs', 431), ('#FullCall', 429), ('#harrystyles', 387), ('#NonStructureFire', 362), ('#', 341), ('#victraffic', 339), ('#Australia', 333), ('#CWC15', 333), ('#TweetMyJobs', 276), ('#australia', 261), ('#FireAlarm', 257), ('#AusOpen', 248), ('#qanda', 248), ('#BBL04', 215), ('#MKR', 198), ('#coffee', 196), ('#summer', 186), ('#love', 183), ('#ausopen', 166), ('#AC2015', 158), ('#mkr', 155), ('#food', 152), ('#ImACelebrityAU', 149), ('#vote5sos', 147), ('#KCA', 141), ('#BusinessMgmt', 113), ('#libspill', 112), ('#ONEDIRECTIONBRIT', 112), ('#QandA', 111), ('#Other', 110), ('#cwc15', 110), ('#Sales', 109), ('#foodporn', 102), ('#art', 101), ('#beach', 100), ('#photo', 98), ('#qldvotes', 96), ('#Hiring', 93), ('#travel', 91), ('#subscribetokianandjc', 90), ('#MacroBusinessLiveTweet', 89), ('#AUSvENG', 88), ('#stkilda', 87)]\n",
      "common words: [('https', 12599), (\"'s\", 11194), ('melbourne', 9956), ('today', 9070), ('wind', 7302), ('humidity', 7268), ('barometer', 7205), (\"n't\", 7111), ('rain', 6853), ('temperature', 6700), ('mm', 6691), ('hpa', 6667), (\"'m\", 6056), ('m/s', 5580), ('a2ï¿½c', 5580), ('...', 4943), ('amp', 4326), ('slowly', 4196), ('love', 3916), ('like', 3822), (\"''\", 3716), ('``', 3689), ('0,0', 3689), ('rising', 3402), ('good', 3363), ('day', 3164), ('get', 3099), ('falling', 3073), ('one', 3068), ('timeline', 2848)]\n"
     ]
    }
   ],
   "source": [
    "for key in tweet_historic_dict.keys():\n",
    "    lst = []\n",
    "    lst_after_removes = []\n",
    "    for text in tweet_historic_dict[key]:\n",
    "        lst = lst + f(text, \"#\")\n",
    "        #if lst: # a list of words contains #\n",
    "            #print(lst)\n",
    "        \n",
    "        removes = set(stopwords.words('english') + list(string.punctuation)+ ['http'])\n",
    "        lst_after_removes = lst_after_removes + [i for i in word_tokenize(text.lower()) if i not in removes]\n",
    "    \n",
    "    c = Counter(lst)\n",
    "    c2 = Counter(lst_after_removes)\n",
    "    print(\"common hash words:\", c.most_common(50))\n",
    "    print(\"common words:\", c2.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80f7e3cd-016f-439f-868f-94772b0fe8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2014', '2015'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_historic_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6fef3bb-96bc-4fe4-8fe0-3f5c89a89d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://t.co/C77LDS8DKr @HipHopDX @DMX #DMX #HipHop #Rap\n",
      "text: http://t.co/C77LDS8DKr @HipHopDX @DMX #DMX #HipHop #Rap\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"http://t.co/C77LDS8DKr @HipHopDX @DMX #DMX #HipHop #Rap\"\n",
    "print(text)\n",
    "lst = f(text, \"#\")\n",
    "        #if lst: # a list of words contains #\n",
    "            #print(lst)\n",
    "lst_after_removes = []       \n",
    "removes = set(stopwords.words('english') + list(string.punctuation)+ ['http'])\n",
    "print(\"text:\", text)\n",
    "lst_after_removes + [i for i in word_tokenize(text.lower()) if i not in removes]\n",
    "lst_after_removes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc480045-862d-46d2-a371-09d55cc7db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "text = 'coming played'\n",
    "porter.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a14d2a-eea5-48df-9411-9eb958f5f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa1e0c-0422-47f5-b033-1102b80f9eee",
   "metadata": {},
   "source": [
    "## timeline data: hash words and words count by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8547d-e405-4a82-b840-21ce3d544c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tweet_historic_dict.keys():\n",
    "    lst = []\n",
    "    lst_after_removes = []\n",
    "    for text in tweet_historic_dict[key]:\n",
    "        lst = lst + f(text, \"#\")\n",
    "        #if lst: # a list of words contains #\n",
    "            #print(lst)\n",
    "        \n",
    "        removes = set(stopwords.words('english') + list(string.punctuation)+ ['http'])\n",
    "        lst_after_removes = lst_after_removes + [i for i in word_tokenize(text.lower()) if i not in removes]\n",
    "    \n",
    "    c = Counter(lst)\n",
    "    c2 = Counter(lst_after_removes)\n",
    "    print(\"common hash words:\", c.most_common(50))\n",
    "    print(\"common words:\", c2.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8177310b-fc24-4bd6-a1e7-703636609f6f",
   "metadata": {},
   "source": [
    "## stream data: hash words and words count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7072f8b8-fd1a-4833-b342-d258831bfd69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
