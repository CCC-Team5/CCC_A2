{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5996dff5-c6f0-458e-a159-bffbb8868a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Indonesian': 94, 'Spanish': 44, 'French': 31, 'Tagalog': 30, 'Japanese': 29, 'Estonian': 25, 'Portuguese': 18, 'Norwegian': 17, 'German': 17, 'Italian': 16, 'Turkish': 12, 'Arabic': 10, 'Danish': 10, 'Haitian': 10, 'Thai': 7, 'Chinese': 6, 'Dutch': 6, 'Finnish': 5, 'Slovenian': 5, 'Russian': 4}\n"
     ]
    }
   ],
   "source": [
    "import couchdb\n",
    "import pandas as pd\n",
    "# from couchdb_settings import *\n",
    "\n",
    "username = 'admin'\n",
    "password = '123456'\n",
    "address = 'localhost:15984'\n",
    "\n",
    "    \n",
    "testdb = couchdb.Database('http://' + address + '/view-test')\n",
    "testdb.resource.credentials = (username, password)\n",
    "\n",
    "\n",
    "languages = {}\n",
    "\n",
    "for item in testdb.view('geo-test/lang-count', group = True, group_level = 1):\n",
    "    if item.key != 'en':\n",
    "        if item.key == 'in':\n",
    "            languages['id'] = item.value\n",
    "        else:\n",
    "            languages[item.key] = item.value\n",
    "            \n",
    "        languages = {k:v for k, v in sorted(languages.items(), key=lambda item: item[1])[::-1][:20]}\n",
    "\n",
    "# print(languages)\n",
    "\n",
    "langCode = {}\n",
    "\n",
    "with open('Data/langCode.json', 'r', encoding= 'utf-8') as f:\n",
    "    for line in f:\n",
    "        (val, key) = line.split()\n",
    "        langCode[key] = val\n",
    "        \n",
    "languages = {v2: v1 for k1, v1 in languages.items() for k2, v2 in langCode.items() if k1 == k2}\n",
    "\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5299d19d-081f-45ab-8e01-5f422a33ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_birth_country(file, N):\n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    match_cols = []\n",
    "    new_cols = []\n",
    "    col_names = data.columns\n",
    "    for name in col_names:\n",
    "        if name.endswith('_p'):\n",
    "            match_cols.append(name)\n",
    "            new_cols.append(name.strip())\n",
    "\n",
    "    ext_data = pd.DataFrame(data[match_cols])\n",
    "    ext_data.columns = new_cols\n",
    "    \n",
    "    grand_total = ext_data['tot_p'].sum(axis = 0)\n",
    "    country_total = ext_data.sum(axis = 0)\n",
    "    percentage = country_total/grand_total * 100\n",
    "    \n",
    "    birth_country = pd.DataFrame(country_total, columns = ['country_total'])\n",
    "    birth_country['percentage'] = percentage\n",
    "    \n",
    "    drop_columns = ['hong_kong_sar_china_p', 'born_elsewhere_p', 'tot_p', 'os_visitors_p', 'country_birth_not_stated_p', 'australia_p', 'new_zealand_p', 'united_states_america_p', 'united_kingdom_ci_im_p', 'fiji_p', 'south_africa_p']\n",
    "    \n",
    "    birth_country.loc['china_excl_sars_taiwan_p'] += birth_country.loc['hong_kong_sar_china_p']\n",
    "    birth_country = birth_country.T.drop(drop_columns, axis = 1)\n",
    "    birth_country = birth_country.rename({'china_excl_sars_taiwan_p' : 'china_p', 'sri_lanka_p' : 'srilanka_p'}, axis = 1)\n",
    "    \n",
    "    country_names = []\n",
    "    for item in birth_country.columns:\n",
    "        item = item.split('_')\n",
    "        country_names.append(item[0].capitalize())\n",
    "\n",
    "    birth_country.columns = country_names\n",
    "    birth_country = birth_country.rename({'Srilanka' : 'Sri Lanka'}, axis = 1).T\n",
    "    birth_country = birth_country.sort_values(by = ['country_total'], ascending = False)[:N]\n",
    "    \n",
    "    return birth_country\n",
    "\n",
    "# top_10_birth_country('Data/AURIN/country_of_birth.csv', 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec145a5-d30b-446b-b917-48da82a21438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_lang_spoken_at_home(file_path, N):\n",
    "    \n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    match_cols = []\n",
    "    new_cols = []\n",
    "    col_names = data.columns\n",
    "    for name in col_names:\n",
    "        if name.endswith('_P'):\n",
    "            match_cols.append(name)   \n",
    "            new_cols.append(name.strip())\n",
    "\n",
    "    ext_data = pd.DataFrame(data[match_cols])\n",
    "    ext_data.columns = new_cols\n",
    "\n",
    "    SOL_tot = ext_data['SOL_Tot_P'].sum(axis = 0)\n",
    "    tot = ext_data['Total_P'].sum(axis = 0)\n",
    "    SOL_perc = SOL_tot/tot * 100\n",
    "\n",
    "    drop_columns = ['SOL_Other_P', 'SOL_Samoan_P', 'SOL_Assyrian_P','SOL_Iran_Lan_Tot_P', \n",
    "                    'SOL_Irani_Lan_Othr_P', 'SOL_Se_As_A_L_Othr_P', 'SOL_Aus_Indig_Lang_P', \n",
    "                    'SOL_In_Ar_Lang_Othr_P', 'SOL_In_Ar_Lang_Tot_P', 'SOL_Se_As_A_L_Tot_P', \n",
    "                    'Language_spoken_home_ns_P', 'SOL_Tot_P', 'Total_P', \n",
    "                    'SOL_Chin_lang_Mand_P', 'SOL_Chin_lang_Other_P', 'SOL_Chin_lang_Cant_P']\n",
    "    \n",
    "    ext_data = ext_data.drop(drop_columns, axis = 1)\n",
    "    lang_tot = ext_data.sum(axis = 0)\n",
    "    \n",
    "    columns = []\n",
    "    for index in lang_tot.index:\n",
    "        idx = index.split('_')\n",
    "        if 'Se_As' in index or 'In_Ar' in index or 'Ir_Lang' in index:\n",
    "            columns.append(idx[-2])\n",
    "        elif 'Ir_La' in index:\n",
    "            columns.append(idx[3])\n",
    "        else:\n",
    "            columns.append(idx[1])\n",
    "\n",
    "    lang_tot.index = columns\n",
    "    lang_data = pd.DataFrame(lang_tot, columns = ['count']).T\n",
    "    lang_data = lang_data.rename({'Pe' : 'Persian'}, axis = 1).T\n",
    "    lang_data['percentage_SOL'] = lang_data['count']/SOL_tot * 100\n",
    "    lang_data['percentage_Total'] = lang_data['count']/tot * 100\n",
    "\n",
    "    langdict = {k:v for v in langCode.values() for k in lang_data.index if k in v}\n",
    "\n",
    "    idx = []\n",
    "    for i in lang_data.index:\n",
    "        name = langdict[i]\n",
    "        idx.append(name)\n",
    "\n",
    "    lang_data.index = idx\n",
    "    lang_data = lang_data.sort_values(by = ['count'], ascending = False)[:N]\n",
    "\n",
    "    return lang_data, SOL_perc\n",
    "    \n",
    "    \n",
    "# top_n_lang_spoken_at_home('Data/AURIN/lang_at_home.csv', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "277706ba-77a1-49c4-b407-a0dfaaa78e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('country_of_birth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "9344c3c8-cd17-473c-8542-98018b5a77a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_cols = []\n",
    "new_cols = []\n",
    "\n",
    "col_names = data.columns\n",
    "\n",
    "for name in col_names:\n",
    "    if name.endswith('_p'):\n",
    "        match_cols.append(name)\n",
    "        new_cols.append(name.strip())\n",
    "\n",
    "ext_data = pd.DataFrame(data[match_cols])\n",
    "ext_data.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "dbd15b40-7203-42b0-abd5-41cd2820a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_total = ext_data['tot_p'].sum(axis = 0)\n",
    "country_total = ext_data.sum(axis = 0)\n",
    "percentage = country_total/grand_total * 100\n",
    "birth_country = pd.DataFrame(country_total, columns = ['country_total'])\n",
    "birth_country['percentage'] = percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e2d0d47a-02e2-4ee9-90c6-77abdbc59458",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_country = birth_country.sort_values(by = ['country_total'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "63e8747e-9357-449d-9100-19d4471bfd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['hong_kong_sar_china_p', 'born_elsewhere_p', 'tot_p', 'os_visitors_p', 'country_birth_not_stated_p', 'australia_p', 'new_zealand_p', 'united_states_america_p', 'united_kingdom_ci_im_p', 'fiji_p', 'south_africa_p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "462946de-b9ef-4f81-83d6-745e871705b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_country.loc['china_excl_sars_taiwan_p'] += birth_country.loc['hong_kong_sar_china_p']\n",
    "birth_country = birth_country.T.drop(drop_columns, axis = 1).T[:10].T\n",
    "birth_country = birth_country.rename({'china_excl_sars_taiwan_p' : 'china_p', 'sri_lanka_p' : 'srilanka_p'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "122c6c18-f637-4162-88d3-f204b68193de",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for item in birth_country.columns:\n",
    "    item = item.split('_')\n",
    "    names.append(item[0].capitalize())\n",
    "\n",
    "birth_country.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "0af98b59-6af6-4fea-b20c-7eada24c1065",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_country = birth_country.rename({'Srilanka' : 'Sri Lanka'}, axis = 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "f13cea2e-75c9-48a9-bc93-403424fb1d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['India', 'China', 'Vietnam', 'Italy', 'Sri Lanka', 'Malaysia', 'Greece',\n",
       "       'Philippines', 'Pakistan', 'Germany'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birth_country.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "2decfdcf-57d7-4c31-962b-104754806bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'India', 'y': 160058.0, 'z': 3.5380284230620616}, {'name': 'China', 'y': 174418.0, 'z': 3.85545140819977}, {'name': 'Vietnam', 'y': 78036.0, 'z': 1.7249596147775874}, {'name': 'Italy', 'y': 61521.0, 'z': 1.3599010772045204}, {'name': 'Sri Lanka', 'y': 52658.0, 'z': 1.1639874339402094}, {'name': 'Malaysia', 'y': 45852.0, 'z': 1.0135430859703463}, {'name': 'Greece', 'y': 43881.0, 'z': 0.9699747918403725}, {'name': 'Philippines', 'y': 43642.0, 'z': 0.9646917769763117}, {'name': 'Pakistan', 'y': 19127.0, 'z': 0.42279592177778097}, {'name': 'Germany', 'y': 17511.0, 'z': 0.38707478361743725}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(len(birth_country)):\n",
    "    entry = {}\n",
    "    e = birth_country.iloc[i]\n",
    "    entry['name'] = e.name\n",
    "    entry['y'] = e.country_total\n",
    "    entry['z'] = e.percentage\n",
    "    results.append(entry)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "73d19680-82fc-46dc-afad-233cb77216f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/AURIN/lang_at_home.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "a477a590-8622-4818-ac8c-7cb083cbb407",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_cols = []\n",
    "new_cols = []\n",
    "\n",
    "col_names = data.columns\n",
    "\n",
    "for name in col_names:\n",
    "    if name.endswith('_P'):\n",
    "        match_cols.append(name)   \n",
    "        new_cols.append(name.strip())\n",
    "\n",
    "ext_data = pd.DataFrame(data[match_cols])\n",
    "ext_data.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "e4a33ada-b5ba-43cc-acd8-8d0c22997fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.072123556525113"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOL_tot = ext_data['SOL_Tot_P'].sum(axis = 0)\n",
    "tot = ext_data['Total_P'].sum(axis = 0)\n",
    "SOL_perc = SOL_tot/tot * 100\n",
    "SOL_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "32d17829-e3c0-4aa5-9f93-45de03cb2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['SOL_Other_P', 'SOL_Samoan_P', 'SOL_Assyrian_P','SOL_Iran_Lan_Tot_P', 'SOL_Irani_Lan_Othr_P', 'SOL_Se_As_A_L_Othr_P', 'SOL_Aus_Indig_Lang_P', 'SOL_In_Ar_Lang_Othr_P', 'SOL_In_Ar_Lang_Tot_P', 'SOL_Se_As_A_L_Tot_P', 'Language_spoken_home_ns_P', 'SOL_Tot_P', 'Total_P', 'SOL_Chin_lang_Mand_P', 'SOL_Chin_lang_Other_P', 'SOL_Chin_lang_Cant_P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "6b816ec2-20c3-4f35-bdc3-20b6245b12eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_data = ext_data.drop(drop_columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "c3ddcef5-1678-470c-aff2-48f3c0c9917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "lang_tot = ext_data.sum(axis = 0)\n",
    "for index in lang_tot.index:\n",
    "    idx = index.split('_')\n",
    "    if 'Se_As' in index or 'In_Ar' in index or 'Ir_Lang' in index:\n",
    "        columns.append(idx[-2])\n",
    "    elif 'Ir_La' in index:\n",
    "        columns.append(idx[3])\n",
    "    else:\n",
    "        columns.append(idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "27127fb4-9aa9-43d6-b2a9-203c0e9775fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_data = pd.DataFrame(lang_tot, columns = ['count']).T\n",
    "lang_data.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "84933565-56f1-42ef-9fc3-8e1eb3291b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_data = lang_data.rename({'Pe' : 'Persian'}, axis = 1).T\n",
    "lang_data['percentage'] = lang_data['count']/SOL_tot * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "49d08413-1739-49cb-8c5c-dc3141868806",
   "metadata": {},
   "outputs": [],
   "source": [
    "langCode = {k:v for v in langCode.values() for k in lang_data.index if k in v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e8fee62c-c421-4769-a860-9e5e1536ec3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'it': 19,\n",
       " 'victoria': 21,\n",
       " 'sales': 23,\n",
       " 'streetart': 24,\n",
       " 'photo': 28,\n",
       " 'coffee': 29,\n",
       " 'spring': 30,\n",
       " 'jobs': 50,\n",
       " 'msfw': 53,\n",
       " 'careerarc': 54,\n",
       " 'hazmat': 54,\n",
       " 'victraffic': 54,\n",
       " 'nonstructurefire': 60,\n",
       " 'incident': 65,\n",
       " 'structurefire': 80,\n",
       " 'hiring': 92,\n",
       " 'firealarm': 95,\n",
       " 'job': 103,\n",
       " 'australia': 108,\n",
       " 'melbourne': 564}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hashtags = {}\n",
    "\n",
    "for item in testdb.view('geo-test/hash-tags', group = True, group_level = 1):\n",
    "    if item.key.lower() not in hashtags.keys():\n",
    "        hashtags[item.key.lower()] = item.value\n",
    "    else:\n",
    "        hashtags[item.key.lower()] += item.value\n",
    "\n",
    "hashtags = {k: v for k, v in sorted(hashtags.items(), key=lambda item: item[1])[-20:]}\n",
    "\n",
    "hashtags\n",
    "\n",
    "# string = ','.join(hashtags)\n",
    "\n",
    "# wc = WordCloud(background_color=\"white\", max_words=100)\n",
    "\n",
    "# plt.imshow(wc.generate(string), interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3b3f70-fc3e-4317-a1a4-20bb1b6e75a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def topic_switch(topic):\n",
    "\n",
    "    # if topic == 'housing':\n",
    "    #     count_view = 'text/housing-count'\n",
    "    #     topic_view = 'text/housing'\n",
    "    # if topic == 'transportation':\n",
    "    #     count_view = 'text/transportation-count'\n",
    "    #     topic_view = 'text/transportation'\n",
    "    # if topic == 'cost':\n",
    "    #     count_view = 'text/cost-count'\n",
    "    #     topic_view = 'text/cost'\n",
    "        \n",
    "        \n",
    "    if topic == 'housing':\n",
    "        count_view = 'geo-test/text-housing-count'\n",
    "        topic_view = 'geo-test/text-housing'\n",
    "    if topic == 'transportation':\n",
    "        count_view = 'geo-test/text-transportation-count'\n",
    "        topic_view = 'geo-test/text-transportation'\n",
    "    if topic == 'cost':\n",
    "        count_view = 'geo-test/text-cost-count'\n",
    "        topic_view = 'geo-test/text-cost'\n",
    "        \n",
    "    return count_view, topic_view\n",
    "\n",
    "def trend_per_topic(db, topic):\n",
    "\n",
    "    #db = testdb\n",
    "    count_view, _ = topic_switch(topic)\n",
    "    \n",
    "    year_topic = {}\n",
    "    year_total = {}\n",
    "    percent = {}\n",
    "\n",
    "    for item in db.view(count_view, group = True, group_level = 1):\n",
    "        year_topic[item.key] = item.value\n",
    "\n",
    "    for item in db.view('time/by-year-count', group = True, group_level = 1):\n",
    "        year_total[item.key] = item.value\n",
    "\n",
    "    for key in year_topic.keys():\n",
    "        if key in year_total.keys():\n",
    "            percent[key] = year_topic[key]/year_total[key] * 100\n",
    "            \n",
    "    return year_topic, year_total, percent\n",
    "\n",
    "\n",
    "def topic_wordcloud(db, topic):\n",
    "\n",
    "    _, topic_view = topic_switch(topic)\n",
    "\n",
    "    yearly_tweets = defaultdict(list)\n",
    "    for item in db.view(topic_view):\n",
    "        yearly_tweets[item.key].append(item.value)\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    \n",
    "    for key, tweet in yearly_tweets.items():\n",
    "        tweet = [' '.join(re.sub(\"(@[A-Za-z0-9\\_]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",t).split()) for t in tweet]\n",
    "        tweet = [' '.join(tweet)]\n",
    "        tweet_tokens = tokenizer.tokenize(tweet[0])\n",
    "        tweet_clean = []\n",
    "        for word in tweet_tokens:\n",
    "            if word.lower() not in stopwords.words('english') and word.lower() not in string.punctuation:\n",
    "                tweet_clean.append(word.lower())\n",
    "                \n",
    "        yearly_tweets[key] = ' '.join(tweet_clean)\n",
    "    \n",
    "    return yearly_tweets\n",
    "\n",
    "# topic_wordcloud(testdb, 'transportation')\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
