{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae57ac82-1b31-4c00-9761-9f9dd6d1d5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Database 'view-test'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bind [127.0.0.1]:15984: Address already in use\n",
      "channel_setup_fwd_listener_tcpip: cannot listen to port: 15984\n",
      "Could not request local forwarding.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import couchdb\n",
    "import pandas as pd\n",
    "# from couchdb_settings import *\n",
    "\n",
    "\n",
    "username = 'admin'\n",
    "password = '123456'\n",
    "address = 'localhost:15984'\n",
    "tweets = 'view-test'\n",
    "\n",
    "testdb = couchdb.Database('http://' + address + '/view-test')\n",
    "testdb.resource.credentials = (username, password)\n",
    "\n",
    "def set_up_db():\n",
    "    os.system(\"chmod 0400 CCC.pem\")\n",
    "    arg = \"ssh -i CCC.pem -L localhost:15984:172.17.0.2:5984 -N -f ec2-user@ec2-3-25-244-154.ap-southeast-2.compute.amazonaws.com\"\n",
    "    proc=subprocess.Popen(arg.split()).wait()\n",
    "    if proc != 0:\n",
    "        print(\"connection is already established\")\n",
    "        pass\n",
    "\n",
    "class CouchDBHandler:\n",
    "    def __init__(self, username, password, address, dbname):\n",
    "        self.db = couchdb.Database('http://' + address + '/' + dbname)\n",
    "        self.db.resource.credentials = (username, password)\n",
    "\n",
    "set_up_db()\n",
    "\n",
    "db_handler = CouchDBHandler(username, password, address, tweets)\n",
    "print(db_handler.db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63b7dc6e-3a24-4e43-9009-0e84f22493dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = {}\n",
    "\n",
    "for item in testdb.view('geo-test/geo-count-historic', group = True):\n",
    "    key = str(item.key)\n",
    "    geo[key] = item.value\n",
    "    \n",
    "# print(geo)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "653cdd67-2336-46cf-9616-b4f28350cfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Indonesian': 94, 'Spanish': 44, 'French': 31, 'Tagalog': 30, 'Japanese': 29, 'Estonian': 25, 'Portuguese': 18, 'Norwegian': 17, 'German': 17, 'Italian': 16, 'Turkish': 12, 'Arabic': 10, 'Danish': 10, 'Haitian': 10, 'Thai': 7, 'Chinese': 6, 'Dutch': 6, 'Finnish': 5, 'Slovenian': 5, 'Russian': 4}\n"
     ]
    }
   ],
   "source": [
    "languages = {}\n",
    "\n",
    "for item in testdb.view('geo-test/lang-count', group = True, group_level = 1):\n",
    "    if item.key != 'en':\n",
    "        if item.key == 'in':\n",
    "            languages['id'] = item.value\n",
    "        else:\n",
    "            languages[item.key] = item.value\n",
    "            \n",
    "        languages = {k:v for k, v in sorted(languages.items(), key=lambda item: item[1])[::-1][:20]}\n",
    "\n",
    "# print(languages)\n",
    "\n",
    "langCode = {}\n",
    "\n",
    "with open('Data/langCode.json', 'r', encoding= 'utf-8') as f:\n",
    "    for line in f:\n",
    "        (val, key) = line.split()\n",
    "        langCode[key] = val\n",
    "        \n",
    "languages = {v2: v1 for k1, v1 in languages.items() for k2, v2 in langCode.items() if k1 == k2}\n",
    "\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5299d19d-081f-45ab-8e01-5f422a33ff88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_total</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>China</th>\n",
       "      <td>174418.0</td>\n",
       "      <td>3.855451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>India</th>\n",
       "      <td>160058.0</td>\n",
       "      <td>3.538028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vietnam</th>\n",
       "      <td>78036.0</td>\n",
       "      <td>1.724960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>61521.0</td>\n",
       "      <td>1.359901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sri Lanka</th>\n",
       "      <td>52658.0</td>\n",
       "      <td>1.163987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malaysia</th>\n",
       "      <td>45852.0</td>\n",
       "      <td>1.013543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greece</th>\n",
       "      <td>43881.0</td>\n",
       "      <td>0.969975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philippines</th>\n",
       "      <td>43642.0</td>\n",
       "      <td>0.964692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pakistan</th>\n",
       "      <td>19127.0</td>\n",
       "      <td>0.422796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>17511.0</td>\n",
       "      <td>0.387075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iraq</th>\n",
       "      <td>17019.0</td>\n",
       "      <td>0.376199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfyrom</th>\n",
       "      <td>15641.0</td>\n",
       "      <td>0.345739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesia</th>\n",
       "      <td>15071.0</td>\n",
       "      <td>0.333139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lebanon</th>\n",
       "      <td>14968.0</td>\n",
       "      <td>0.330863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iran</th>\n",
       "      <td>14551.0</td>\n",
       "      <td>0.321645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malta</th>\n",
       "      <td>14230.0</td>\n",
       "      <td>0.314549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkey</th>\n",
       "      <td>13646.0</td>\n",
       "      <td>0.301640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Singapore</th>\n",
       "      <td>13479.0</td>\n",
       "      <td>0.297949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korea</th>\n",
       "      <td>13133.0</td>\n",
       "      <td>0.290301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thailand</th>\n",
       "      <td>11945.0</td>\n",
       "      <td>0.264040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             country_total  percentage\n",
       "China             174418.0    3.855451\n",
       "India             160058.0    3.538028\n",
       "Vietnam            78036.0    1.724960\n",
       "Italy              61521.0    1.359901\n",
       "Sri Lanka          52658.0    1.163987\n",
       "Malaysia           45852.0    1.013543\n",
       "Greece             43881.0    0.969975\n",
       "Philippines        43642.0    0.964692\n",
       "Pakistan           19127.0    0.422796\n",
       "Germany            17511.0    0.387075\n",
       "Iraq               17019.0    0.376199\n",
       "Tfyrom             15641.0    0.345739\n",
       "Indonesia          15071.0    0.333139\n",
       "Lebanon            14968.0    0.330863\n",
       "Iran               14551.0    0.321645\n",
       "Malta              14230.0    0.314549\n",
       "Turkey             13646.0    0.301640\n",
       "Singapore          13479.0    0.297949\n",
       "Korea              13133.0    0.290301\n",
       "Thailand           11945.0    0.264040"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_10_birth_country(file, N):\n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    match_cols = []\n",
    "    new_cols = []\n",
    "    col_names = data.columns\n",
    "    for name in col_names:\n",
    "        if name.endswith('_p'):\n",
    "            match_cols.append(name)\n",
    "            new_cols.append(name.strip())\n",
    "\n",
    "    ext_data = pd.DataFrame(data[match_cols])\n",
    "    ext_data.columns = new_cols\n",
    "    \n",
    "    grand_total = ext_data['tot_p'].sum(axis = 0)\n",
    "    country_total = ext_data.sum(axis = 0)\n",
    "    percentage = country_total/grand_total * 100\n",
    "    \n",
    "    birth_country = pd.DataFrame(country_total, columns = ['country_total'])\n",
    "    birth_country['percentage'] = percentage\n",
    "    \n",
    "    drop_columns = ['hong_kong_sar_china_p', 'born_elsewhere_p', 'tot_p', 'os_visitors_p', 'country_birth_not_stated_p', 'australia_p', 'new_zealand_p', 'united_states_america_p', 'united_kingdom_ci_im_p', 'fiji_p', 'south_africa_p']\n",
    "    \n",
    "    birth_country.loc['china_excl_sars_taiwan_p'] += birth_country.loc['hong_kong_sar_china_p']\n",
    "    birth_country = birth_country.T.drop(drop_columns, axis = 1)\n",
    "    birth_country = birth_country.rename({'china_excl_sars_taiwan_p' : 'china_p', 'sri_lanka_p' : 'srilanka_p'}, axis = 1)\n",
    "    \n",
    "    country_names = []\n",
    "    for item in birth_country.columns:\n",
    "        item = item.split('_')\n",
    "        country_names.append(item[0].capitalize())\n",
    "\n",
    "    birth_country.columns = country_names\n",
    "    birth_country = birth_country.rename({'Srilanka' : 'Sri Lanka'}, axis = 1).T\n",
    "    birth_country = birth_country.sort_values(by = ['country_total'], ascending = False)[:N]\n",
    "    \n",
    "    return birth_country\n",
    "\n",
    "top_10_birth_country('Data/AURIN/country_of_birth.csv', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ec145a5-d30b-446b-b917-48da82a21438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(             count  percentage_SOL  percentage_Total\n",
       " Chinese     189854       16.326190          4.746370\n",
       " Greek       113424        9.753715          2.835612\n",
       " Italian     112665        9.688446          2.816637\n",
       " Vietnamese   85122        7.319930          2.128059\n",
       " Arabic       65454        5.628612          1.636357\n",
       " Hindi        31607        2.717993          0.790178\n",
       " Turkish      31257        2.687896          0.781428\n",
       " Punjabi      29517        2.538267          0.737928\n",
       " Macedonian   29378        2.526314          0.734453\n",
       " Spanish      27632        2.376170          0.690803\n",
       " Sinhala      27439        2.359573          0.685978\n",
       " Croatian     18855        1.621405          0.471377\n",
       " Maltese      17219        1.480720          0.430477\n",
       " Tamil        16907        1.453890          0.422677\n",
       " Serbian      16247        1.397135          0.406177\n",
       " German       16246        1.397049          0.406152\n",
       " Russian      15863        1.364113          0.396577\n",
       " Polish       15197        1.306842          0.379927\n",
       " Tagalog      15136        1.301596          0.378402\n",
       " French       14916        1.282677          0.372902,\n",
       " 29.072123556525113)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_n_lang_spoken_at_home(file_path, N):\n",
    "    \n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    match_cols = []\n",
    "    new_cols = []\n",
    "    col_names = data.columns\n",
    "    for name in col_names:\n",
    "        if name.endswith('_P'):\n",
    "            match_cols.append(name)   \n",
    "            new_cols.append(name.strip())\n",
    "\n",
    "    ext_data = pd.DataFrame(data[match_cols])\n",
    "    ext_data.columns = new_cols\n",
    "\n",
    "    SOL_tot = ext_data['SOL_Tot_P'].sum(axis = 0)\n",
    "    tot = ext_data['Total_P'].sum(axis = 0)\n",
    "    SOL_perc = SOL_tot/tot * 100\n",
    "\n",
    "    drop_columns = ['SOL_Other_P', 'SOL_Samoan_P', 'SOL_Assyrian_P','SOL_Iran_Lan_Tot_P', \n",
    "                    'SOL_Irani_Lan_Othr_P', 'SOL_Se_As_A_L_Othr_P', 'SOL_Aus_Indig_Lang_P', \n",
    "                    'SOL_In_Ar_Lang_Othr_P', 'SOL_In_Ar_Lang_Tot_P', 'SOL_Se_As_A_L_Tot_P', \n",
    "                    'Language_spoken_home_ns_P', 'SOL_Tot_P', 'Total_P', \n",
    "                    'SOL_Chin_lang_Mand_P', 'SOL_Chin_lang_Other_P', 'SOL_Chin_lang_Cant_P']\n",
    "    \n",
    "    ext_data = ext_data.drop(drop_columns, axis = 1)\n",
    "    lang_tot = ext_data.sum(axis = 0)\n",
    "    \n",
    "    columns = []\n",
    "    for index in lang_tot.index:\n",
    "        idx = index.split('_')\n",
    "        if 'Se_As' in index or 'In_Ar' in index or 'Ir_Lang' in index:\n",
    "            columns.append(idx[-2])\n",
    "        elif 'Ir_La' in index:\n",
    "            columns.append(idx[3])\n",
    "        else:\n",
    "            columns.append(idx[1])\n",
    "\n",
    "    lang_tot.index = columns\n",
    "    lang_data = pd.DataFrame(lang_tot, columns = ['count']).T\n",
    "    lang_data = lang_data.rename({'Pe' : 'Persian'}, axis = 1).T\n",
    "    lang_data['percentage_SOL'] = lang_data['count']/SOL_tot * 100\n",
    "    lang_data['percentage_Total'] = lang_data['count']/tot * 100\n",
    "\n",
    "    langdict = {k:v for v in langCode.values() for k in lang_data.index if k in v}\n",
    "\n",
    "    idx = []\n",
    "    for i in lang_data.index:\n",
    "        name = langdict[i]\n",
    "        idx.append(name)\n",
    "\n",
    "    lang_data.index = idx\n",
    "    lang_data = lang_data.sort_values(by = ['count'], ascending = False)[:N]\n",
    "\n",
    "    return lang_data, SOL_perc\n",
    "    \n",
    "    \n",
    "top_n_lang_spoken_at_home('Data/AURIN/lang_at_home.csv', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "277706ba-77a1-49c4-b407-a0dfaaa78e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('country_of_birth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "9344c3c8-cd17-473c-8542-98018b5a77a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_cols = []\n",
    "new_cols = []\n",
    "\n",
    "col_names = data.columns\n",
    "\n",
    "for name in col_names:\n",
    "    if name.endswith('_p'):\n",
    "        match_cols.append(name)\n",
    "        new_cols.append(name.strip())\n",
    "\n",
    "ext_data = pd.DataFrame(data[match_cols])\n",
    "ext_data.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "dbd15b40-7203-42b0-abd5-41cd2820a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_total = ext_data['tot_p'].sum(axis = 0)\n",
    "country_total = ext_data.sum(axis = 0)\n",
    "percentage = country_total/grand_total * 100\n",
    "birth_country = pd.DataFrame(country_total, columns = ['country_total'])\n",
    "birth_country['percentage'] = percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e2d0d47a-02e2-4ee9-90c6-77abdbc59458",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_country = birth_country.sort_values(by = ['country_total'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "63e8747e-9357-449d-9100-19d4471bfd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['hong_kong_sar_china_p', 'born_elsewhere_p', 'tot_p', 'os_visitors_p', 'country_birth_not_stated_p', 'australia_p', 'new_zealand_p', 'united_states_america_p', 'united_kingdom_ci_im_p', 'fiji_p', 'south_africa_p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "462946de-b9ef-4f81-83d6-745e871705b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_country.loc['china_excl_sars_taiwan_p'] += birth_country.loc['hong_kong_sar_china_p']\n",
    "birth_country = birth_country.T.drop(drop_columns, axis = 1).T[:10].T\n",
    "birth_country = birth_country.rename({'china_excl_sars_taiwan_p' : 'china_p', 'sri_lanka_p' : 'srilanka_p'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "122c6c18-f637-4162-88d3-f204b68193de",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for item in birth_country.columns:\n",
    "    item = item.split('_')\n",
    "    names.append(item[0].capitalize())\n",
    "\n",
    "birth_country.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "0af98b59-6af6-4fea-b20c-7eada24c1065",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_country = birth_country.rename({'Srilanka' : 'Sri Lanka'}, axis = 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "f13cea2e-75c9-48a9-bc93-403424fb1d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['India', 'China', 'Vietnam', 'Italy', 'Sri Lanka', 'Malaysia', 'Greece',\n",
       "       'Philippines', 'Pakistan', 'Germany'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birth_country.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "2decfdcf-57d7-4c31-962b-104754806bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'India', 'y': 160058.0, 'z': 3.5380284230620616}, {'name': 'China', 'y': 174418.0, 'z': 3.85545140819977}, {'name': 'Vietnam', 'y': 78036.0, 'z': 1.7249596147775874}, {'name': 'Italy', 'y': 61521.0, 'z': 1.3599010772045204}, {'name': 'Sri Lanka', 'y': 52658.0, 'z': 1.1639874339402094}, {'name': 'Malaysia', 'y': 45852.0, 'z': 1.0135430859703463}, {'name': 'Greece', 'y': 43881.0, 'z': 0.9699747918403725}, {'name': 'Philippines', 'y': 43642.0, 'z': 0.9646917769763117}, {'name': 'Pakistan', 'y': 19127.0, 'z': 0.42279592177778097}, {'name': 'Germany', 'y': 17511.0, 'z': 0.38707478361743725}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(len(birth_country)):\n",
    "    entry = {}\n",
    "    e = birth_country.iloc[i]\n",
    "    entry['name'] = e.name\n",
    "    entry['y'] = e.country_total\n",
    "    entry['z'] = e.percentage\n",
    "    results.append(entry)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "73d19680-82fc-46dc-afad-233cb77216f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/AURIN/lang_at_home.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "a477a590-8622-4818-ac8c-7cb083cbb407",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_cols = []\n",
    "new_cols = []\n",
    "\n",
    "col_names = data.columns\n",
    "\n",
    "for name in col_names:\n",
    "    if name.endswith('_P'):\n",
    "        match_cols.append(name)   \n",
    "        new_cols.append(name.strip())\n",
    "\n",
    "ext_data = pd.DataFrame(data[match_cols])\n",
    "ext_data.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "e4a33ada-b5ba-43cc-acd8-8d0c22997fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.072123556525113"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOL_tot = ext_data['SOL_Tot_P'].sum(axis = 0)\n",
    "tot = ext_data['Total_P'].sum(axis = 0)\n",
    "SOL_perc = SOL_tot/tot * 100\n",
    "SOL_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "32d17829-e3c0-4aa5-9f93-45de03cb2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['SOL_Other_P', 'SOL_Samoan_P', 'SOL_Assyrian_P','SOL_Iran_Lan_Tot_P', 'SOL_Irani_Lan_Othr_P', 'SOL_Se_As_A_L_Othr_P', 'SOL_Aus_Indig_Lang_P', 'SOL_In_Ar_Lang_Othr_P', 'SOL_In_Ar_Lang_Tot_P', 'SOL_Se_As_A_L_Tot_P', 'Language_spoken_home_ns_P', 'SOL_Tot_P', 'Total_P', 'SOL_Chin_lang_Mand_P', 'SOL_Chin_lang_Other_P', 'SOL_Chin_lang_Cant_P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "6b816ec2-20c3-4f35-bdc3-20b6245b12eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_data = ext_data.drop(drop_columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "c3ddcef5-1678-470c-aff2-48f3c0c9917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "lang_tot = ext_data.sum(axis = 0)\n",
    "for index in lang_tot.index:\n",
    "    idx = index.split('_')\n",
    "    if 'Se_As' in index or 'In_Ar' in index or 'Ir_Lang' in index:\n",
    "        columns.append(idx[-2])\n",
    "    elif 'Ir_La' in index:\n",
    "        columns.append(idx[3])\n",
    "    else:\n",
    "        columns.append(idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "27127fb4-9aa9-43d6-b2a9-203c0e9775fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_data = pd.DataFrame(lang_tot, columns = ['count']).T\n",
    "lang_data.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "84933565-56f1-42ef-9fc3-8e1eb3291b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_data = lang_data.rename({'Pe' : 'Persian'}, axis = 1).T\n",
    "lang_data['percentage'] = lang_data['count']/SOL_tot * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "49d08413-1739-49cb-8c5c-dc3141868806",
   "metadata": {},
   "outputs": [],
   "source": [
    "langCode = {k:v for v in langCode.values() for k in lang_data.index if k in v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8fee62c-c421-4769-a860-9e5e1536ec3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'it': 19,\n",
       " 'victoria': 21,\n",
       " 'sales': 23,\n",
       " 'streetart': 24,\n",
       " 'photo': 28,\n",
       " 'coffee': 29,\n",
       " 'spring': 30,\n",
       " 'jobs': 50,\n",
       " 'msfw': 53,\n",
       " 'careerarc': 54,\n",
       " 'hazmat': 54,\n",
       " 'victraffic': 54,\n",
       " 'nonstructurefire': 60,\n",
       " 'incident': 65,\n",
       " 'structurefire': 80,\n",
       " 'hiring': 92,\n",
       " 'firealarm': 95,\n",
       " 'job': 103,\n",
       " 'australia': 108,\n",
       " 'melbourne': 564}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hashtags = {}\n",
    "\n",
    "for item in testdb.view('geo-test/hash-tags', group = True, group_level = 1):\n",
    "    if item.key.lower() not in hashtags.keys():\n",
    "        hashtags[item.key.lower()] = item.value\n",
    "    else:\n",
    "        hashtags[item.key.lower()] += item.value\n",
    "\n",
    "hashtags = {k: v for k, v in sorted(hashtags.items(), key=lambda item: item[1])[-20:]}\n",
    "\n",
    "hashtags\n",
    "\n",
    "# string = ','.join(hashtags)\n",
    "\n",
    "# wc = WordCloud(background_color=\"white\", max_words=100)\n",
    "\n",
    "# plt.imshow(wc.generate(string), interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a3b3f70-fc3e-4317-a1a4-20bb1b6e75a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {2015: 'guy refused accept private property preceded formation formal legal systems love pragmatism unsure point complex systems impossible without private property legal sys isnt initiated rule law derived respect private property fail yourdaap understanding came first law private property actually serious implications far trivial hammer tong love name love cuisine lot wasted real estate difficult replace good real estate brokers technology spring air integrity real estate integrityrealestate yarravalley yarraglen kinglake new york real estate queen dolly lenz reivstateconference 2015 dollylenzrealestate wbpproperty'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def topic_switch(topic):\n",
    "\n",
    "    # if topic == 'housing':\n",
    "    #     count_view = 'text/housing-count'\n",
    "    #     topic_view = 'text/housing'\n",
    "    # if topic == 'transportation':\n",
    "    #     count_view = 'text/transportation-count'\n",
    "    #     topic_view = 'text/transportation'\n",
    "    # if topic == 'cost':\n",
    "    #     count_view = 'text/cost-count'\n",
    "    #     topic_view = 'text/cost'\n",
    "        \n",
    "        \n",
    "    if topic == 'housing':\n",
    "        count_view = 'geo-test/text-housing-count'\n",
    "        topic_view = 'geo-test/text-housing'\n",
    "    if topic == 'transportation':\n",
    "        count_view = 'geo-test/text-transportation-count'\n",
    "        topic_view = 'geo-test/text-transportation'\n",
    "    if topic == 'cost':\n",
    "        count_view = 'geo-test/text-cost-count'\n",
    "        topic_view = 'geo-test/text-cost'\n",
    "        \n",
    "    return count_view, topic_view\n",
    "\n",
    "def trend_per_topic(db, topic):\n",
    "\n",
    "    #db = testdb\n",
    "    count_view, _ = topic_switch(topic)\n",
    "    \n",
    "    year_topic = {}\n",
    "    year_total = {}\n",
    "    percent = {}\n",
    "\n",
    "    for item in db.view(count_view, group = True, group_level = 1):\n",
    "        year_topic[item.key] = item.value\n",
    "\n",
    "    for item in db.view('time/by-year-count', group = True, group_level = 1):\n",
    "        year_total[item.key] = item.value\n",
    "\n",
    "    for key in year_topic.keys():\n",
    "        if key in year_total.keys():\n",
    "            percent[key] = year_topic[key]/year_total[key] * 100\n",
    "            \n",
    "    return year_topic, year_total, percent\n",
    "\n",
    "\n",
    "def topic_wordcloud(db, topic):\n",
    "\n",
    "    _, topic_view = topic_switch(topic)\n",
    "\n",
    "    yearly_tweets = defaultdict(list)\n",
    "    for item in db.view(topic_view):\n",
    "        yearly_tweets[item.key].append(item.value)\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    \n",
    "    for key, tweet in yearly_tweets.items():\n",
    "        tweet = [' '.join(re.sub(\"(@[A-Za-z0-9\\_]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",t).split()) for t in tweet]\n",
    "        tweet = [' '.join(tweet)]\n",
    "        tweet_tokens = tokenizer.tokenize(tweet[0])\n",
    "        tweet_clean = []\n",
    "        for word in tweet_tokens:\n",
    "            if word.lower() not in stopwords.words('english') and word.lower() not in string.punctuation:\n",
    "                tweet_clean.append(word.lower())\n",
    "                \n",
    "        yearly_tweets[key] = ' '.join(tweet_clean)\n",
    "    \n",
    "    return yearly_tweets\n",
    "\n",
    "topic_wordcloud(testdb, 'housing')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d19e6f11-c63b-4a43-aab3-b99a67629f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2015: -0.020908143939393962}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def topic_sentiment(db, topic):\n",
    "\n",
    "    yearly_tweets = topic_wordcloud(db, topic)\n",
    "    \n",
    "    yearly_sentiment = {}\n",
    "    for key, value in yearly_tweets.items():\n",
    "        blob = TextBlob(value)\n",
    "        for sentence in blob.sentences:\n",
    "            sentiment = sentence.sentiment.polarity\n",
    "            yearly_sentiment[key] = sentiment\n",
    "    return yearly_sentiment\n",
    "\n",
    "topic_sentiment(testdb, 'transportation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
