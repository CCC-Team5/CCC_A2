{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae57ac82-1b31-4c00-9761-9f9dd6d1d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import couchdb\n",
    "import pandas as pd\n",
    "# from couchdb_settings import *\n",
    "\n",
    "\n",
    "username = 'grp5admin'\n",
    "password = 'password'\n",
    "address = '172.26.130.201:5984'\n",
    "tweets = 'raw_tweets'\n",
    "\n",
    "tweet_db = couchdb.Database('http://' + address + '/' + tweets)\n",
    "tweet_db.resource.credentials = (username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b7dc6e-3a24-4e43-9009-0e84f22493dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testdb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zm/w0wn08v14ggc6v0qq8qzjmz80000gn/T/ipykernel_20857/2905726240.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgeo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'geo-test/geo-count-historic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgeo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testdb' is not defined"
     ]
    }
   ],
   "source": [
    "geo = {}\n",
    "\n",
    "for item in testdb.view('geo-test/geo-count-historic', group = True):\n",
    "    key = str(item.key)\n",
    "    geo[key] = item.value\n",
    "    \n",
    "# print(geo)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "653cdd67-2336-46cf-9616-b4f28350cfd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testdb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zm/w0wn08v14ggc6v0qq8qzjmz80000gn/T/ipykernel_20857/4085487887.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlanguages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'geo-test/lang-count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'in'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testdb' is not defined"
     ]
    }
   ],
   "source": [
    "languages = {}\n",
    "\n",
    "for item in testdb.view('geo-test/lang-count', group = True, group_level = 1):\n",
    "    if item.key != 'en':\n",
    "        if item.key == 'in':\n",
    "            languages['id'] = item.value\n",
    "        else:\n",
    "            languages[item.key] = item.value\n",
    "            \n",
    "        languages = {k:v for k, v in sorted(languages.items(), key=lambda item: item[1])[::-1][:20]}\n",
    "\n",
    "# print(languages)\n",
    "\n",
    "langCode = {}\n",
    "\n",
    "with open('Data/langCode.json', 'r', encoding= 'utf-8') as f:\n",
    "    for line in f:\n",
    "        (val, key) = line.split()\n",
    "        langCode[key] = val\n",
    "        \n",
    "languages = {v2: v1 for k1, v1 in languages.items() for k2, v2 in langCode.items() if k1 == k2}\n",
    "\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5299d19d-081f-45ab-8e01-5f422a33ff88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_total</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>China</th>\n",
       "      <td>174418.0</td>\n",
       "      <td>3.855451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>India</th>\n",
       "      <td>160058.0</td>\n",
       "      <td>3.538028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vietnam</th>\n",
       "      <td>78036.0</td>\n",
       "      <td>1.724960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>61521.0</td>\n",
       "      <td>1.359901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sri Lanka</th>\n",
       "      <td>52658.0</td>\n",
       "      <td>1.163987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malaysia</th>\n",
       "      <td>45852.0</td>\n",
       "      <td>1.013543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greece</th>\n",
       "      <td>43881.0</td>\n",
       "      <td>0.969975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Philippines</th>\n",
       "      <td>43642.0</td>\n",
       "      <td>0.964692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pakistan</th>\n",
       "      <td>19127.0</td>\n",
       "      <td>0.422796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>17511.0</td>\n",
       "      <td>0.387075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iraq</th>\n",
       "      <td>17019.0</td>\n",
       "      <td>0.376199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tfyrom</th>\n",
       "      <td>15641.0</td>\n",
       "      <td>0.345739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indonesia</th>\n",
       "      <td>15071.0</td>\n",
       "      <td>0.333139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lebanon</th>\n",
       "      <td>14968.0</td>\n",
       "      <td>0.330863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iran</th>\n",
       "      <td>14551.0</td>\n",
       "      <td>0.321645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malta</th>\n",
       "      <td>14230.0</td>\n",
       "      <td>0.314549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turkey</th>\n",
       "      <td>13646.0</td>\n",
       "      <td>0.301640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Singapore</th>\n",
       "      <td>13479.0</td>\n",
       "      <td>0.297949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korea</th>\n",
       "      <td>13133.0</td>\n",
       "      <td>0.290301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thailand</th>\n",
       "      <td>11945.0</td>\n",
       "      <td>0.264040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             country_total  percentage\n",
       "China             174418.0    3.855451\n",
       "India             160058.0    3.538028\n",
       "Vietnam            78036.0    1.724960\n",
       "Italy              61521.0    1.359901\n",
       "Sri Lanka          52658.0    1.163987\n",
       "Malaysia           45852.0    1.013543\n",
       "Greece             43881.0    0.969975\n",
       "Philippines        43642.0    0.964692\n",
       "Pakistan           19127.0    0.422796\n",
       "Germany            17511.0    0.387075\n",
       "Iraq               17019.0    0.376199\n",
       "Tfyrom             15641.0    0.345739\n",
       "Indonesia          15071.0    0.333139\n",
       "Lebanon            14968.0    0.330863\n",
       "Iran               14551.0    0.321645\n",
       "Malta              14230.0    0.314549\n",
       "Turkey             13646.0    0.301640\n",
       "Singapore          13479.0    0.297949\n",
       "Korea              13133.0    0.290301\n",
       "Thailand           11945.0    0.264040"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_10_birth_country(file, N):\n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    match_cols = []\n",
    "    new_cols = []\n",
    "    col_names = data.columns\n",
    "    for name in col_names:\n",
    "        if name.endswith('_p'):\n",
    "            match_cols.append(name)\n",
    "            new_cols.append(name.strip())\n",
    "\n",
    "    ext_data = pd.DataFrame(data[match_cols])\n",
    "    ext_data.columns = new_cols\n",
    "    \n",
    "    grand_total = ext_data['tot_p'].sum(axis = 0)\n",
    "    country_total = ext_data.sum(axis = 0)\n",
    "    percentage = country_total/grand_total * 100\n",
    "    \n",
    "    birth_country = pd.DataFrame(country_total, columns = ['country_total'])\n",
    "    birth_country['percentage'] = percentage\n",
    "    \n",
    "    drop_columns = ['hong_kong_sar_china_p', 'born_elsewhere_p', 'tot_p', 'os_visitors_p', 'country_birth_not_stated_p', 'australia_p', 'new_zealand_p', 'united_states_america_p', 'united_kingdom_ci_im_p', 'fiji_p', 'south_africa_p']\n",
    "    \n",
    "    birth_country.loc['china_excl_sars_taiwan_p'] += birth_country.loc['hong_kong_sar_china_p']\n",
    "    birth_country = birth_country.T.drop(drop_columns, axis = 1)\n",
    "    birth_country = birth_country.rename({'china_excl_sars_taiwan_p' : 'china_p', 'sri_lanka_p' : 'srilanka_p'}, axis = 1)\n",
    "    \n",
    "    country_names = []\n",
    "    for item in birth_country.columns:\n",
    "        item = item.split('_')\n",
    "        country_names.append(item[0].capitalize())\n",
    "\n",
    "    birth_country.columns = country_names\n",
    "    birth_country = birth_country.rename({'Srilanka' : 'Sri Lanka'}, axis = 1).T\n",
    "    birth_country = birth_country.sort_values(by = ['country_total'], ascending = False)[:N]\n",
    "    \n",
    "    return birth_country\n",
    "\n",
    "top_10_birth_country('Data/AURIN/country_of_birth.csv', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec145a5-d30b-446b-b917-48da82a21438",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'langCode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zm/w0wn08v14ggc6v0qq8qzjmz80000gn/T/ipykernel_20857/3776366450.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mtop_n_lang_spoken_at_home\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/AURIN/lang_at_home.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/zm/w0wn08v14ggc6v0qq8qzjmz80000gn/T/ipykernel_20857/3776366450.py\u001b[0m in \u001b[0;36mtop_n_lang_spoken_at_home\u001b[0;34m(file_path, N)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mlang_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'percentage_Total'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtot\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mlangdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlangCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlang_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'langCode' is not defined"
     ]
    }
   ],
   "source": [
    "def top_n_lang_spoken_at_home(file_path, N):\n",
    "    \n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    match_cols = []\n",
    "    new_cols = []\n",
    "    col_names = data.columns\n",
    "    for name in col_names:\n",
    "        if name.endswith('_P'):\n",
    "            match_cols.append(name)   \n",
    "            new_cols.append(name.strip())\n",
    "\n",
    "    ext_data = pd.DataFrame(data[match_cols])\n",
    "    ext_data.columns = new_cols\n",
    "\n",
    "    SOL_tot = ext_data['SOL_Tot_P'].sum(axis = 0)\n",
    "    tot = ext_data['Total_P'].sum(axis = 0)\n",
    "    SOL_perc = SOL_tot/tot * 100\n",
    "\n",
    "    drop_columns = ['SOL_Other_P', 'SOL_Samoan_P', 'SOL_Assyrian_P','SOL_Iran_Lan_Tot_P', \n",
    "                    'SOL_Irani_Lan_Othr_P', 'SOL_Se_As_A_L_Othr_P', 'SOL_Aus_Indig_Lang_P', \n",
    "                    'SOL_In_Ar_Lang_Othr_P', 'SOL_In_Ar_Lang_Tot_P', 'SOL_Se_As_A_L_Tot_P', \n",
    "                    'Language_spoken_home_ns_P', 'SOL_Tot_P', 'Total_P', \n",
    "                    'SOL_Chin_lang_Mand_P', 'SOL_Chin_lang_Other_P', 'SOL_Chin_lang_Cant_P']\n",
    "    \n",
    "    ext_data = ext_data.drop(drop_columns, axis = 1)\n",
    "    lang_tot = ext_data.sum(axis = 0)\n",
    "    \n",
    "    columns = []\n",
    "    for index in lang_tot.index:\n",
    "        idx = index.split('_')\n",
    "        if 'Se_As' in index or 'In_Ar' in index or 'Ir_Lang' in index:\n",
    "            columns.append(idx[-2])\n",
    "        elif 'Ir_La' in index:\n",
    "            columns.append(idx[3])\n",
    "        else:\n",
    "            columns.append(idx[1])\n",
    "\n",
    "    lang_tot.index = columns\n",
    "    lang_data = pd.DataFrame(lang_tot, columns = ['number']).T\n",
    "    lang_data = lang_data.rename({'Pe' : 'Persian'}, axis = 1).T\n",
    "    lang_data['percentage_SOL'] = lang_data['number']/SOL_tot * 100\n",
    "    lang_data['percentage_Total'] = lang_data['number']/tot * 100\n",
    "\n",
    "    langdict = {k:v for v in langCode.values() for k in lang_data.index if k in v}\n",
    "\n",
    "    idx = []\n",
    "    for i in lang_data.index:\n",
    "        name = langdict[i]\n",
    "        idx.append(name)\n",
    "\n",
    "    lang_data.index = idx\n",
    "    lang_data = lang_data.sort_values(by = ['number'], ascending = False)[:N]\n",
    "    \n",
    "    spoken = {}\n",
    "    for i in range(len(lang_data)):\n",
    "        spoken[lang_data.index[i]] = lang_data.number[i], lang_data.percentage_SOL[i], lang_data.percentage_Total[i]\n",
    "\n",
    "    return spoken\n",
    "    \n",
    "    \n",
    "top_n_lang_spoken_at_home('Data/AURIN/lang_at_home.csv', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "277706ba-77a1-49c4-b407-a0dfaaa78e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('country_of_birth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "9344c3c8-cd17-473c-8542-98018b5a77a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_cols = []\n",
    "new_cols = []\n",
    "\n",
    "col_names = data.columns\n",
    "\n",
    "for name in col_names:\n",
    "    if name.endswith('_p'):\n",
    "        match_cols.append(name)\n",
    "        new_cols.append(name.strip())\n",
    "\n",
    "ext_data = pd.DataFrame(data[match_cols])\n",
    "ext_data.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "dbd15b40-7203-42b0-abd5-41cd2820a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_total = ext_data['tot_p'].sum(axis = 0)\n",
    "country_total = ext_data.sum(axis = 0)\n",
    "percentage = country_total/grand_total * 100\n",
    "birth_country = pd.DataFrame(country_total, columns = ['country_total'])\n",
    "birth_country['percentage'] = percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e2d0d47a-02e2-4ee9-90c6-77abdbc59458",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_country = birth_country.sort_values(by = ['country_total'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "63e8747e-9357-449d-9100-19d4471bfd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['hong_kong_sar_china_p', 'born_elsewhere_p', 'tot_p', 'os_visitors_p', 'country_birth_not_stated_p', 'australia_p', 'new_zealand_p', 'united_states_america_p', 'united_kingdom_ci_im_p', 'fiji_p', 'south_africa_p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "462946de-b9ef-4f81-83d6-745e871705b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_country.loc['china_excl_sars_taiwan_p'] += birth_country.loc['hong_kong_sar_china_p']\n",
    "birth_country = birth_country.T.drop(drop_columns, axis = 1).T[:10].T\n",
    "birth_country = birth_country.rename({'china_excl_sars_taiwan_p' : 'china_p', 'sri_lanka_p' : 'srilanka_p'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "122c6c18-f637-4162-88d3-f204b68193de",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for item in birth_country.columns:\n",
    "    item = item.split('_')\n",
    "    names.append(item[0].capitalize())\n",
    "\n",
    "birth_country.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "0af98b59-6af6-4fea-b20c-7eada24c1065",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_country = birth_country.rename({'Srilanka' : 'Sri Lanka'}, axis = 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "f13cea2e-75c9-48a9-bc93-403424fb1d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['India', 'China', 'Vietnam', 'Italy', 'Sri Lanka', 'Malaysia', 'Greece',\n",
       "       'Philippines', 'Pakistan', 'Germany'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birth_country.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "2decfdcf-57d7-4c31-962b-104754806bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'India', 'y': 160058.0, 'z': 3.5380284230620616}, {'name': 'China', 'y': 174418.0, 'z': 3.85545140819977}, {'name': 'Vietnam', 'y': 78036.0, 'z': 1.7249596147775874}, {'name': 'Italy', 'y': 61521.0, 'z': 1.3599010772045204}, {'name': 'Sri Lanka', 'y': 52658.0, 'z': 1.1639874339402094}, {'name': 'Malaysia', 'y': 45852.0, 'z': 1.0135430859703463}, {'name': 'Greece', 'y': 43881.0, 'z': 0.9699747918403725}, {'name': 'Philippines', 'y': 43642.0, 'z': 0.9646917769763117}, {'name': 'Pakistan', 'y': 19127.0, 'z': 0.42279592177778097}, {'name': 'Germany', 'y': 17511.0, 'z': 0.38707478361743725}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(len(birth_country)):\n",
    "    entry = {}\n",
    "    e = birth_country.iloc[i]\n",
    "    entry['name'] = e.name\n",
    "    entry['y'] = e.country_total\n",
    "    entry['z'] = e.percentage\n",
    "    results.append(entry)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "73d19680-82fc-46dc-afad-233cb77216f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/AURIN/lang_at_home.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "a477a590-8622-4818-ac8c-7cb083cbb407",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_cols = []\n",
    "new_cols = []\n",
    "\n",
    "col_names = data.columns\n",
    "\n",
    "for name in col_names:\n",
    "    if name.endswith('_P'):\n",
    "        match_cols.append(name)   \n",
    "        new_cols.append(name.strip())\n",
    "\n",
    "ext_data = pd.DataFrame(data[match_cols])\n",
    "ext_data.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "e4a33ada-b5ba-43cc-acd8-8d0c22997fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.072123556525113"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOL_tot = ext_data['SOL_Tot_P'].sum(axis = 0)\n",
    "tot = ext_data['Total_P'].sum(axis = 0)\n",
    "SOL_perc = SOL_tot/tot * 100\n",
    "SOL_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "32d17829-e3c0-4aa5-9f93-45de03cb2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['SOL_Other_P', 'SOL_Samoan_P', 'SOL_Assyrian_P','SOL_Iran_Lan_Tot_P', 'SOL_Irani_Lan_Othr_P', 'SOL_Se_As_A_L_Othr_P', 'SOL_Aus_Indig_Lang_P', 'SOL_In_Ar_Lang_Othr_P', 'SOL_In_Ar_Lang_Tot_P', 'SOL_Se_As_A_L_Tot_P', 'Language_spoken_home_ns_P', 'SOL_Tot_P', 'Total_P', 'SOL_Chin_lang_Mand_P', 'SOL_Chin_lang_Other_P', 'SOL_Chin_lang_Cant_P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "6b816ec2-20c3-4f35-bdc3-20b6245b12eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_data = ext_data.drop(drop_columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "c3ddcef5-1678-470c-aff2-48f3c0c9917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "lang_tot = ext_data.sum(axis = 0)\n",
    "for index in lang_tot.index:\n",
    "    idx = index.split('_')\n",
    "    if 'Se_As' in index or 'In_Ar' in index or 'Ir_Lang' in index:\n",
    "        columns.append(idx[-2])\n",
    "    elif 'Ir_La' in index:\n",
    "        columns.append(idx[3])\n",
    "    else:\n",
    "        columns.append(idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "27127fb4-9aa9-43d6-b2a9-203c0e9775fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_data = pd.DataFrame(lang_tot, columns = ['count']).T\n",
    "lang_data.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "84933565-56f1-42ef-9fc3-8e1eb3291b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_data = lang_data.rename({'Pe' : 'Persian'}, axis = 1).T\n",
    "lang_data['percentage'] = lang_data['count']/SOL_tot * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "49d08413-1739-49cb-8c5c-dc3141868806",
   "metadata": {},
   "outputs": [],
   "source": [
    "langCode = {k:v for v in langCode.values() for k in lang_data.index if k in v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8fee62c-c421-4769-a860-9e5e1536ec3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testdb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zm/w0wn08v14ggc6v0qq8qzjmz80000gn/T/ipykernel_20857/2061853037.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhashtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'geo-test/hash-tags'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhashtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mhashtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testdb' is not defined"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hashtags = {}\n",
    "\n",
    "for item in testdb.view('geo-test/hash-tags', group = True, group_level = 1):\n",
    "    if item.key.lower() not in hashtags.keys():\n",
    "        hashtags[item.key.lower()] = item.value\n",
    "    else:\n",
    "        hashtags[item.key.lower()] += item.value\n",
    "\n",
    "hashtags = {k: v for k, v in sorted(hashtags.items(), key=lambda item: item[1])[-20:]}\n",
    "\n",
    "hashtags\n",
    "\n",
    "# string = ','.join(hashtags)\n",
    "\n",
    "# wc = WordCloud(background_color=\"white\", max_words=100)\n",
    "\n",
    "# plt.imshow(wc.generate(string), interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a3b3f70-fc3e-4317-a1a4-20bb1b6e75a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def topic_switch(topic):\n",
    "    \"\"\"\n",
    "    params: topic of selection\n",
    "    return: paths to views relating to the selected topic \n",
    "    \"\"\"\n",
    "\n",
    "    if topic == 'housing':\n",
    "        count_view = 'text/housing-count'\n",
    "        topic_view = 'text/housing'\n",
    "        topic_db_text = db_connect('housing_text')\n",
    "    if topic == 'transportation':\n",
    "        count_view = 'text/transportation-count'\n",
    "        topic_view = 'text/transportation'\n",
    "        topic_db_text = db_connect('trans_text')\n",
    "    if topic == 'cost':\n",
    "        count_view = 'text/cost-count'\n",
    "        topic_view = 'text/cost'\n",
    "        topic_db_text = db_connect('cost_text')\n",
    "\n",
    "    return count_view, topic_view, topic_db_text\n",
    "\n",
    "# def trend_per_topic(db, topic):\n",
    "\n",
    "#     #db = testdb\n",
    "#     count_view, _ = topic_switch(topic)\n",
    "    \n",
    "#     year_topic = {}\n",
    "#     year_total = {}\n",
    "#     percent = {}\n",
    "\n",
    "#     for item in db.view(count_view, group = True, group_level = 1):\n",
    "#         year_topic[item.key] = item.value\n",
    "\n",
    "#     for item in db.view('geo-test/by-year-count', group = True, group_level = 1):\n",
    "#         year_total[item.key] = item.value\n",
    "\n",
    "#     for key in year_topic.keys():\n",
    "#         if key in year_total.keys():\n",
    "#             percent[key] = year_topic[key]/year_total[key] * 100\n",
    "            \n",
    "#     return year_topic, year_total, percent\n",
    "\n",
    "def delete_docs(topic, save_db):\n",
    "    docs = []    \n",
    "    for row in save_db.view(topic + '/all', include_docs=True):\n",
    "        doc = row['doc']\n",
    "        if int(doc['year'] >=2018):\n",
    "            doc['_deleted']=True\n",
    "            docs.append(doc)\n",
    "        save_db.update(docs)    \n",
    "\n",
    "def topic_wordcloud(query_db, topic):\n",
    "    \"\"\"\n",
    "    Extract topic related wordcloud\n",
    "    params: raw_tweets database;\n",
    "            topic of selection\n",
    "    return: corpus of combined tweets on the selected topic indexed by year; \n",
    "            all words in lowercases\n",
    "    return type: dict - {year : corpus as a list}\n",
    "    render: wordcloud\n",
    "    \"\"\"\n",
    "\n",
    "    _, topic_view, save_db = topic_switch(topic)\n",
    "    \n",
    "    delete_docs(topic, save_db)\n",
    "\n",
    "    yearly_tweets = defaultdict(list)\n",
    "    for item in query_db.view(topic_view):\n",
    "        yearly_tweets[item.key].append(item.value)\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    \n",
    "    for key, tweet in yearly_tweets.items():\n",
    "        tweet = [' '.join(re.sub(\"(@[A-Za-z0-9\\_]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",t).split()) for t in tweet]\n",
    "        tweet = [' '.join(tweet)]\n",
    "        tweet_tokens = tokenizer.tokenize(tweet[0])\n",
    "        tweet_clean = []\n",
    "        for word in tweet_tokens:\n",
    "            if word.lower() not in stopwords.words('english') and word.lower() not in string.punctuation:\n",
    "                tweet_clean.append(word.lower())\n",
    "                \n",
    "        yearly_tweets[key] = ' '.join(tweet_clean)\n",
    "        \n",
    "    \n",
    "    for k, v in yearly_tweets.items():\n",
    "        save_db.save({'year': k, 'text':v})\n",
    "            \n",
    "\n",
    "\n",
    "def db_connect(dbname):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    couchserver = couchdb.Server('http://' + username + ':' + password + '@' + address)\n",
    "    try:\n",
    "      db = couchserver[dbname]\n",
    "    except:\n",
    "      db = couchserver.create(dbname)\n",
    "\n",
    "    return db\n",
    "\n",
    "\n",
    "for topic in ['cost', 'housing', 'transportation']:\n",
    "    topic_wordcloud(tweet_db, topic)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac4f58a-5ac7-405a-bb83-c2987438c3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({2015: 8}, {2015: 10623, 2018: 3, 2022: 3}, {2015: 0.07530829332580251})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trend_per_topic(testdb, 'housing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d19e6f11-c63b-4a43-aab3-b99a67629f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2018: 0.26719977553310886,\n",
       " 2019: 0.05319404621352675,\n",
       " 2020: 0.08493145743145744,\n",
       " 2021: 0.06922077922077922,\n",
       " 2022: -0.014355069812816458}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def topic_sentiment(topic):\n",
    "\n",
    "    _, _, db = topic_switch(topic)\n",
    "    \n",
    "    yearly_tweets = {}\n",
    "    for item in db.view(topic + '/text'):\n",
    "        yearly_tweets[item.key] = item.value\n",
    "    \n",
    "    yearly_sentiment = {}\n",
    "    for key, value in yearly_tweets.items():\n",
    "        blob = TextBlob(value)\n",
    "        for sentence in blob.sentences:\n",
    "            sentiment = sentence.sentiment.polarity\n",
    "            yearly_sentiment[key] = sentiment\n",
    "            \n",
    "    return yearly_sentiment\n",
    "\n",
    "topic_sentiment('transportation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
